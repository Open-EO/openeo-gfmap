{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of a GFMAP full-extraction pipeline\n",
    "\n",
    "Designing a full-extraction pipeline using the openeo-gfmap GFMAPJobManager.\n",
    "\n",
    "The pipeline consits of the following elements:\n",
    "* An input dataframe where each row corresponds to each executed job.\n",
    "* An user function defined to create OpenEO BatchJob from input rows of the beforementioned dataframe.\n",
    "* An user function defined to generate an output path of each of the Job products\n",
    "* An user function executed after the assets are downloaded and saved from a finished job (optional). This **post job action** can do anything and will be executed locally inside the GFMAPJobManager.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the logging module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the logging for the openeo_gfmap package\n",
    "from openeo_gfmap.manager import _log\n",
    "import logging\n",
    "\n",
    "_log.setLevel(logging.DEBUG)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "_log.addHandler(stream_handler)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s|%(name)s|%(levelname)s:  %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "\n",
    "# Exclude the other loggers from other libraries\n",
    "class MyLoggerFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return record.name == _log.name\n",
    "\n",
    "stream_handler.addFilter(MyLoggerFilter())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step: splitting the job\n",
    "\n",
    "We load the initial crop-type dataset that will be the base of our extractions.\n",
    "\n",
    "Splitting the dataset of extraction in multiple job based on position is necessary to respect OpenEO limitations.\n",
    "\n",
    "This script performs a split with the H3 hexagonal grid, yielding a list of sub-geodataframes.\n",
    "\n",
    "A subtility here is that some polygons are not directly extracted (field with `extract=False`), but should be kept for post-job actions. This requirement is filled by removing sub-dataframes that do not contain any extractable polyons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/users/Private/couchard/openeo-gfmap/src/openeo_gfmap/manager/job_splitters.py:83: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  geom_col = polygons.geometry.centroid\n",
      "/data/users/Private/couchard/openeo-gfmap/src/openeo_gfmap/manager/job_splitters.py:60: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  polygons[\"geometry\"] = polygons.geometry.centroid\n",
      "/data/users/Private/couchard/openeo-gfmap/src/openeo_gfmap/manager/job_splitters.py:64: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  s2_grid[\"geometry\"] = s2_grid.geometry.centroid\n",
      "/home/couchard/miniconda3/envs/gfmap/lib/python3.9/site-packages/geopandas/array.py:365: UserWarning: Geometry is in a geographic CRS. Results from 'sjoin_nearest' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270 sub-datasets.\n",
      "222 sub-datasets after filtering sub-datasets with no point to extract.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "from openeo_gfmap.manager.job_splitters import split_job_hex, split_job_s2grid, _append_h3_index\n",
    "\n",
    "base_df_path = \"https://artifactory.vgt.vito.be/artifactory/auxdata-public/gfmap/DEMO_CROPTYPE.gpkg\"\n",
    "base_df = gpd.read_file(base_df_path)\n",
    "base_df = _append_h3_index(base_df, grid_resolution=3)\n",
    "# Splits the job using GFMAP\n",
    "split_jobs = split_job_s2grid(\n",
    "    base_df, max_points=60\n",
    ")\n",
    "\n",
    "print(f'{len(split_jobs)} sub-datasets.')\n",
    "\n",
    "# Remove the geometry where there are no points with the \"extract\" flag\n",
    "split_jobs = [\n",
    "    job for job in split_jobs if job.extract.any()\n",
    "]\n",
    "print(f'{len(split_jobs)} sub-datasets after filtering sub-datasets with no point to extract.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second step: creating a dataframe for the GFMAP Job Manager\n",
    "\n",
    "Implementing a function that yields a `pandas.DataFrame` where each row correponds to a job.\n",
    "\n",
    "The dataframe should contain the informations required by the GFMAP Job Manager, as well as additional information used by the datacube creation function and the post-job action function.\n",
    "\n",
    "The output dataframe should be savable as a .csv file.\n",
    "\n",
    "Note: the full information of a sub-geodataframe of polygons can be saved into a row of a `pandas.DataFrame` by storing it in a row as string implementing the `geojson.FeatureCollection` interface. To convert the `geopandas.GeoDataFrame` into a stirng, simply use the `.to_json()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backend_name</th>\n",
       "      <th>out_prefix</th>\n",
       "      <th>out_extension</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>s2_tile</th>\n",
       "      <th>h3index</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2-L2A-10m</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>31UDS</td>\n",
       "      <td>83194dfffffffff</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2-L2A-10m</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>31UES</td>\n",
       "      <td>831fa4fffffffff</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2-L2A-10m</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>31UES</td>\n",
       "      <td>83194dfffffffff</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2-L2A-10m</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>31UFS</td>\n",
       "      <td>831fa4fffffffff</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2-L2A-10m</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>31UFS</td>\n",
       "      <td>831fa4fffffffff</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2-L2A-10m</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>35VND</td>\n",
       "      <td>831f6cfffffffff</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2-L2A-10m</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>35VND</td>\n",
       "      <td>831f6cfffffffff</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2-L2A-10m</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>35VND</td>\n",
       "      <td>831f6cfffffffff</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2-L2A-10m</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>35VND</td>\n",
       "      <td>831f6cfffffffff</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2-L2A-10m</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>35VND</td>\n",
       "      <td>831f6cfffffffff</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>222 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    backend_name  out_prefix out_extension  start_date    end_date s2_tile  \\\n",
       "0           cdse  S2-L2A-10m           .nc  2020-08-30  2022-03-03   31UDS   \n",
       "1           cdse  S2-L2A-10m           .nc  2020-08-30  2022-03-03   31UES   \n",
       "2           cdse  S2-L2A-10m           .nc  2020-08-30  2022-03-03   31UES   \n",
       "3           cdse  S2-L2A-10m           .nc  2020-08-30  2022-03-03   31UFS   \n",
       "4           cdse  S2-L2A-10m           .nc  2020-08-30  2022-03-03   31UFS   \n",
       "..           ...         ...           ...         ...         ...     ...   \n",
       "217         cdse  S2-L2A-10m           .nc  2020-08-30  2022-03-03   35VND   \n",
       "218         cdse  S2-L2A-10m           .nc  2020-08-30  2022-03-03   35VND   \n",
       "219         cdse  S2-L2A-10m           .nc  2020-08-30  2022-03-03   35VND   \n",
       "220         cdse  S2-L2A-10m           .nc  2020-08-30  2022-03-03   35VND   \n",
       "221         cdse  S2-L2A-10m           .nc  2020-08-30  2022-03-03   35VND   \n",
       "\n",
       "             h3index                                           geometry  \n",
       "0    83194dfffffffff  {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "1    831fa4fffffffff  {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "2    83194dfffffffff  {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "3    831fa4fffffffff  {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "4    831fa4fffffffff  {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "..               ...                                                ...  \n",
       "217  831f6cfffffffff  {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "218  831f6cfffffffff  {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "219  831f6cfffffffff  {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "220  831f6cfffffffff  {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "221  831f6cfffffffff  {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "\n",
       "[222 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openeo_gfmap import Backend\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "def create_job_dataframe_s2(backend: Backend, split_jobs: List[gpd.GeoDataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"Create a dataframe from the split jobs, containg all the necessary information to run the job.\"\"\"\n",
    "    columns = ['backend_name', 'out_prefix', 'out_extension', 'start_date', 'end_date', 's2_tile', 'h3index', 'geometry']\n",
    "    rows = []\n",
    "    for job in split_jobs:\n",
    "        # Compute the average in the valid date and make a buffer of 1.5 year around\n",
    "        median_time = pd.to_datetime(job.valid_date).mean()\n",
    "        start_date = median_time - pd.Timedelta(days=275)  # A bit more than 9 months\n",
    "        end_date = median_time + pd.Timedelta(days=275)  # A bit more than 9 months\n",
    "        s2_tile = job.tile.iloc[0]  # Job dataframes are split depending on the\n",
    "        h3index = job.h3index.iloc[0] \n",
    "        \n",
    "        rows.append(\n",
    "            pd.Series(\n",
    "                dict(zip(columns, [backend.value, 'S2-L2A-10m', '.nc',  start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'), s2_tile, h3index, job.to_json()]))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "job_df = create_job_dataframe_s2(Backend.CDSE, split_jobs)\n",
    "\n",
    "job_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-sampling job dataframe to reduce execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a subset of the jobs to test the manager, the selected jobs have a fair amount of geometries to extract\n",
    "# 8, 6, 4, 2 and 1 tiles\n",
    "job_df = job_df.iloc[[0]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backend_name</th>\n",
       "      <th>out_prefix</th>\n",
       "      <th>out_extension</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>s2_tile</th>\n",
       "      <th>h3index</th>\n",
       "      <th>geometry</th>\n",
       "      <th>nb_polygons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2-L2A-10m</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>31UDS</td>\n",
       "      <td>83194dfffffffff</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  backend_name  out_prefix out_extension  start_date    end_date s2_tile  \\\n",
       "0         cdse  S2-L2A-10m           .nc  2020-08-30  2022-03-03   31UDS   \n",
       "\n",
       "           h3index                                           geometry  \\\n",
       "0  83194dfffffffff  {\"type\": \"FeatureCollection\", \"features\": [{\"i...   \n",
       "\n",
       "   nb_polygons  \n",
       "0            1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geojson\n",
    "\n",
    "def get_job_nb_polygons(row: pd.Series) -> int:\n",
    "    \"\"\"Get the number of polygons in the geometry.\"\"\"\n",
    "    return len(list(filter(lambda feat: feat.properties.get(\"extract\"), geojson.loads(row.geometry)['features'])))\n",
    "\n",
    "job_df['nb_polygons'] = job_df.apply(get_job_nb_polygons, axis=1)\n",
    "job_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third step: implement the datacube creator function.\n",
    "\n",
    "Implement a function to create, from the additional rows provided before, an `openeo.BatchJob` that will be used to run the job.\n",
    "\n",
    "In this case we extract Sentinel-2 data around a 64x64 pixel square of polygons which have the field `extract=True` (although we keep them in the row for the post-job action.)\n",
    "\n",
    "Note:\n",
    "Because the polygons to extract are specified in UTM dimensions (required to have a specific size), the dataset of polygon cannot be send directly through the openeo process graph (GeoJSON only support lat/lon coordinates). The sub-datasets of polygons are therefore uploaded to a publicly accessible URL so they can be used later by openeo during the execution of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openeo\n",
    "\n",
    "import requests\n",
    "from tempfile import NamedTemporaryFile\n",
    "import os\n",
    "import pandas as pd\n",
    "import geojson\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from openeo_gfmap import TemporalContext, Backend, BackendContext, FetchType, SpatialContext\n",
    "from openeo_gfmap.fetching import build_sentinel2_l2a_extractor\n",
    "\n",
    "def upload_geoparquet_artifactory(gdf: gpd.GeoDataFrame, row_id: int) -> str:\n",
    "    # Save the dataframe as geoparquet to upload it to artifactory\n",
    "    temporary_file = NamedTemporaryFile()\n",
    "    gdf.to_parquet(temporary_file.name)\n",
    "    \n",
    "    artifactory_username = os.getenv('ARTIFACTORY_USERNAME')\n",
    "    artifactory_password = os.getenv('ARTIFACTORY_PASSWORD')\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/octet-stream\"\n",
    "    }\n",
    "\n",
    "    upload_url = f\"https://artifactory.vgt.vito.be/artifactory/auxdata-public/gfmap-temp/openeogfmap_dataframe_{row_id}.parquet\"\n",
    "\n",
    "    with open(temporary_file.name, 'rb') as f:\n",
    "        response = requests.put(upload_url, headers=headers, data=f, auth=(artifactory_username, artifactory_password))\n",
    "\n",
    "    assert response.status_code == 201, f\"Error uploading the dataframe to artifactory: {response.text}\"\n",
    "\n",
    "    return upload_url\n",
    "\n",
    "\n",
    "def create_datacube_s2(row: pd.Series, connection: openeo.DataCube, provider=None, connection_provider=None) -> openeo.BatchJob:\n",
    "\n",
    "    def buffer_geometry(geometry: geojson.FeatureCollection, buffer: int) -> gpd.GeoDataFrame:\n",
    "        gdf = gpd.GeoDataFrame.from_features(geometry).set_crs(epsg=4326)\n",
    "        utm = gdf.estimate_utm_crs()\n",
    "        gdf = gdf.to_crs(utm)\n",
    "\n",
    "        gdf['geometry'] = gdf.centroid.apply(\n",
    "            # Clips the point to the closest 20m from the S2 grid\n",
    "            lambda point: Point(round(point.x / 20.0) * 20.0, round(point.y / 20.0) * 20.0)\n",
    "        ).buffer(distance=buffer, cap_style=3)\n",
    "\n",
    "        return gdf\n",
    "\n",
    "    def filter_extractonly_geometries(collection: geojson.FeatureCollection):\n",
    "        # Filter out geometries that do not have the field extract=True\n",
    "        features = [f for f in collection.features if f.properties.get('extract', False)]\n",
    "        return geojson.FeatureCollection(features)\n",
    "\n",
    "    start_date = row.start_date\n",
    "    end_date = row.end_date\n",
    "    temporal_context = TemporalContext(start_date, end_date)\n",
    "\n",
    "    # Get the feature collection containing the geometry to the job\n",
    "    geometry = geojson.loads(row.geometry)\n",
    "    assert isinstance(geometry, geojson.FeatureCollection)\n",
    "\n",
    "    # Filter the geometry to the rows with the extract only flag\n",
    "    geometry = filter_extractonly_geometries(geometry)\n",
    "    assert len(geometry.features) > 0, \"No geometries with the extract flag found\"\n",
    "\n",
    "    # Performs a buffer of 64 px around the geometry\n",
    "    geometry_df = buffer_geometry(geometry, 320)\n",
    "    spatial_extent_url = upload_geoparquet_artifactory(geometry_df, row.name)\n",
    "\n",
    "    # Backend name and fetching type\n",
    "    backend = Backend(row.backend_name)\n",
    "    backend_context = BackendContext(backend)\n",
    "\n",
    "    fetch_type = FetchType.POLYGON\n",
    "    bands_to_download = ['S2-L2A-B01', 'S2-L2A-B02', 'S2-L2A-B03', 'S2-L2A-B04', 'S2-L2A-B05', 'S2-L2A-B06', 'S2-L2A-B07', 'S2-L2A-B08', 'S2-L2A-B8A', 'S2-L2A-B09', 'S2-L2A-B11', 'S2-L2A-B12', 'S2-L2A-SCL']\n",
    "\n",
    "    # Create the job to extract S2\n",
    "    extraction_parameters = {\n",
    "        \"target_resolution\": 10,\n",
    "        \"load_collection\": {\n",
    "            \"eo:cloud_cover\": lambda val: val <= 95.0,\n",
    "            \"tileId\": lambda val: val == row.s2_tile\n",
    "        },\n",
    "    }\n",
    "    extractor = build_sentinel2_l2a_extractor(\n",
    "        backend_context, bands=bands_to_download, fetch_type=fetch_type.POLYGON, **extraction_parameters \n",
    "    )\n",
    "\n",
    "    cube = extractor.get_cube(connection, spatial_extent_url, temporal_context)\n",
    "\n",
    "    # Compute the SCL dilation and add it to the cube\n",
    "    scl_dilated_mask = cube.process(\n",
    "        \"to_scl_dilation_mask\",\n",
    "        data=cube,\n",
    "        scl_band_name=\"S2-L2A-SCL\",\n",
    "        kernel1_size=17,  # 17px dilation on a 20m layer\n",
    "        kernel2_size=77,   # 77px dilation on a 20m layer\n",
    "        mask1_values=[2, 4, 5, 6, 7],\n",
    "        mask2_values=[3, 8, 9, 10, 11],\n",
    "        erosion_kernel_size=3\n",
    "    ).rename_labels(\"bands\", [\"S2-L2A-SCL_DILATED_MASK\"])\n",
    "\n",
    "    cube = cube.merge_cubes(scl_dilated_mask)\n",
    "    cube = cube.linear_scale_range(0, 65534, 0, 65534)\n",
    "\n",
    "    # Get the h3index to use in the tile\n",
    "    s2_tile = row.s2_tile\n",
    "    valid_date = geometry.features[0].properties['valid_date']\n",
    "\n",
    "    # Increase the memory of the jobs depending on the number of polygons to extract\n",
    "    number_polygons = get_job_nb_polygons(row)\n",
    "    _log.debug(f\"Number of polygons to extract: {number_polygons}\")\n",
    "\n",
    "    job_options = {\n",
    "        \"executor-memory\": \"5G\",\n",
    "        \"executor-memoryOverhead\": \"2G\",\n",
    "    }\n",
    "\n",
    "    return cube.create_job(\n",
    "        out_format=\"NetCDF\",\n",
    "        title=f\"GFMAP_Extraction_S2_{s2_tile}_{valid_date}\",\n",
    "        sample_by_feature=True,\n",
    "        job_options=job_options\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth step: create output paths\n",
    "\n",
    "Implement a function that from the sample index the job row determines which path to saves the assets to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openeo_gfmap.manager.job_splitters import _load_s2_grid\n",
    "\n",
    "# Load the S2 grid\n",
    "s2_grid = _load_s2_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "\n",
    "def generate_output_path_s2(root_folder: Path, geometry_index: int, row: pd.Series):\n",
    "    features = geojson.loads(row.geometry)\n",
    "    sample_id = features[geometry_index].properties['sample_id']\n",
    "    ref_id = features[geometry_index].properties['ref_id']\n",
    "    \n",
    "    s2_tile_id = row.s2_tile\n",
    "    h3index = row.h3index\n",
    "    \n",
    "    subfolder = root_folder / ref_id / h3index / sample_id\n",
    "    return subfolder / f'{row.out_prefix}_{sample_id}_{h3index}_{row.start_date}_{row.end_date}{row.out_extension}'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fifth step: Define the post-job action\n",
    "\n",
    "The post-job action will be called once the job resut was downloaded and saved to a specific path.\n",
    "\n",
    "A post-job action function must receive 3 parameters:\n",
    "* `job_items`: STAC items containing the currently extracted data for the job.\n",
    "* `row`: The current job dataframe row.\n",
    "* `parameters`: User-defined parameters set in the `GFMAPJobManager` constructor.\n",
    "\n",
    "The post-job action must return back the list of job items. The user is responsible for updating,\n",
    "adding and removing the items. For example, in this case the user creates a raster file with\n",
    "ground truth data, the user then adds an asset using the predefined `openeo_gfmap.stac.AUXILIARY`\n",
    "AssetDefintion to the related item, pointing to the generated NetCDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:20: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:20: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "/tmp/ipykernel_5325/4293936125.py:20: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert (\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from importlib.metadata import version\n",
    "from datetime import datetime\n",
    "\n",
    "import pystac\n",
    "from pyproj import CRS\n",
    "from shapely.geometry import box\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.transform import from_bounds\n",
    "from openeo_gfmap.stac import AUXILIARY\n",
    "\n",
    "def add_item_asset(related_item: pystac.Item, path: Path):\n",
    "    asset = AUXILIARY.create_asset(\n",
    "        href=path.as_posix()\n",
    "    )\n",
    "    related_item.add_asset('auxiliary', asset)\n",
    "\n",
    "def post_job_action(job_items: List[pystac.Item], row: pd.Series, parameters: dict = {}) -> list:\n",
    "    base_gpd = gpd.GeoDataFrame.from_features(json.loads(row.geometry)).set_crs(epsg=4326)\n",
    "    assert len(base_gpd[base_gpd.extract == True]) == len(job_items), \"The number of result paths should be the same as the number of geometries\"\n",
    "    extracted_gpd = base_gpd[base_gpd.extract == True].reset_index(drop=True)\n",
    "    # In this case we want to burn the metadata in a new file in the same folder as the S2 product\n",
    "    for idx, item in enumerate(job_items):\n",
    "        sample_id = extracted_gpd.iloc[idx].sample_id\n",
    "        ref_id = extracted_gpd.iloc[idx].ref_id\n",
    "        confidence = extracted_gpd.iloc[idx].confidence\n",
    "        valid_date = extracted_gpd.iloc[idx].valid_date\n",
    "\n",
    "        item_asset_path = Path(\n",
    "            list(item.assets.values())[0].href\n",
    "        )\n",
    "        # Read information from the item file (could also read it from the item object metadata)\n",
    "        result_ds = xr.open_dataset(item_asset_path, chunks='auto')\n",
    "\n",
    "        # Add some metadata to the result_df netcdf file\n",
    "        result_ds.attrs.update({\n",
    "            'start_date': row.start_date,\n",
    "            'end_date': row.end_date,\n",
    "            'valid_date': valid_date,\n",
    "            'GFMAP_version': version('openeo_gfmap'),\n",
    "            'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'description': f'Sentinel2 L2A observations for sample: {sample_id}, unprocessed.',\n",
    "            'title': f'Sentinel2 L2A - {sample_id}',\n",
    "            'sample_id': sample_id,\n",
    "            'spatial_resolution': '10m'\n",
    "        })\n",
    "        result_ds.to_netcdf(item_asset_path, format='NETCDF4', engine='h5netcdf')\n",
    "\n",
    "        target_crs = CRS.from_wkt(result_ds.crs.attrs['crs_wkt'])\n",
    "\n",
    "        # Get the surrounding polygons around our extracted center geometry to rastetize them\n",
    "        bounds = (\n",
    "            result_ds.x.min().item(),\n",
    "            result_ds.y.min().item(),\n",
    "            result_ds.x.max().item(),\n",
    "            result_ds.y.max().item()\n",
    "        )\n",
    "        bbox = box(*bounds)\n",
    "        surround_gpd = base_gpd.to_crs(target_crs).clip(bbox)\n",
    "\n",
    "        # Burn the polygon croptypes\n",
    "        transform = from_bounds(*bounds, result_ds.x.size, result_ds.y.size)\n",
    "        croptype_shapes = list(zip(surround_gpd.geometry, surround_gpd.croptype_label))\n",
    "\n",
    "        fill_value = 0\n",
    "        croptype = rasterize(\n",
    "            croptype_shapes,\n",
    "            out_shape=(result_ds.y.size, result_ds.x.size),\n",
    "            transform=transform,\n",
    "            all_touched=False,\n",
    "            fill=fill_value,\n",
    "            default_value=0,\n",
    "            dtype='int64'\n",
    "        )\n",
    "\n",
    "        # Create the attributes to add to the metadata\n",
    "        attributes = {\n",
    "            'ref_id': ref_id,\n",
    "            'sample_id': sample_id,\n",
    "            'confidence': str(confidence),\n",
    "            'valid_date': valid_date,\n",
    "            '_FillValue': fill_value,\n",
    "            'Conventions': 'CF-1.9',\n",
    "            'GFMAP_version': version('openeo_gfmap'),\n",
    "            'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'description': f'Contains rasterized WorldCereal labels for sample: {sample_id}.',\n",
    "            'title': f'WORLDCEREAL Auxiliary file for sample: {sample_id}',\n",
    "            'spatial_resolution': '10m'\n",
    "        }\n",
    "\n",
    "        aux_dataset = xr.Dataset(\n",
    "            {'LABEL': (('y', 'x'), croptype), 'crs': result_ds['crs']},\n",
    "            coords={'y': result_ds.y, 'x': result_ds.x},\n",
    "            attrs=attributes\n",
    "        )\n",
    "        # Required to map the 'crs' layer as geo-reference for the 'LABEL' layer.\n",
    "        aux_dataset['LABEL'].attrs['grid_mapping'] = 'crs'\n",
    "\n",
    "        # Save the metadata in the same folder as the S2 product\n",
    "        metadata_path = (\n",
    "            item_asset_path.parent / \n",
    "            f'WORLDCEREAL_10m_{sample_id}_{target_crs.to_epsg()}_{valid_date}.nc'\n",
    "        )\n",
    "        aux_dataset.to_netcdf(metadata_path, format='NETCDF4', engine='h5netcdf')\n",
    "        aux_dataset.close()\n",
    "\n",
    "        # Adds this metadata as a new asset\n",
    "        add_item_asset(item, metadata_path)\n",
    "        \n",
    "    return job_items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sixth and last step: Running the manager\n",
    "\n",
    "Let's initialize and execute the Job Manager as defined the GFMAP, and then run it using the functions defined previously\n",
    "\n",
    "STAC related parameters such as `collection_id` and `collection_description` are also required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openeo_gfmap.manager.job_manager import GFMAPJobManager\n",
    "from openeo_gfmap.backend import cdse_staging_connection\n",
    "\n",
    "\n",
    "base_output_dir = Path('/data/users/Public/couchard/world_cereal/extractions_bys2tile_2/')\n",
    "tracking_job_csv = base_output_dir / 'job_tracker.csv'\n",
    "\n",
    "manager = GFMAPJobManager(\n",
    "    output_dir=base_output_dir,\n",
    "    output_path_generator=generate_output_path_s2,\n",
    "    collection_id='SENTINEL2-EXTRACTION',\n",
    "    collection_description=(\n",
    "        \"Sentinel-2 and Auxiliary data extraction example.\"\n",
    "    ),\n",
    "    post_job_action=post_job_action,\n",
    "    poll_sleep=60,\n",
    "    n_threads=2,\n",
    "    post_job_params={}\n",
    ")\n",
    "\n",
    "manager.add_backend(\n",
    "    Backend.CDSE_STAGING.value, cdse_staging_connection, parallel_jobs=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 11:48:56,822|openeo_gfmap.manager|INFO:  Starting job manager using 2 worker threads.\n",
      "2024-03-14 11:48:56,824|openeo_gfmap.manager|INFO:  Workers started, creating and running jobs.\n",
      "2024-03-14 11:48:56,829|openeo_gfmap.manager|DEBUG:  Normalizing dataframe. Columns: Index(['backend_name', 'out_prefix', 'out_extension', 'start_date', 'end_date',\n",
      "       's2_tile', 'h3index', 'geometry', 'nb_polygons', 'status', 'id',\n",
      "       'start_time', 'cpu', 'memory', 'duration', 'description', 'costs'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated using refresh token.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/couchard/miniconda3/envs/gfmap/lib/python3.9/site-packages/openeo/rest/connection.py:1188: UserWarning: SENTINEL2_L2A property filtering with properties that are undefined in the collection metadata (summaries): tileId.\n",
      "  return DataCube.load_collection(\n",
      "2024-03-14 11:48:59,295|openeo_gfmap.manager|DEBUG:  Number of polygons to extract: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataCube(<PGNode 'dimension_labels' at 0x7f1f4db08f40>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 11:50:15,120|openeo_gfmap.manager|DEBUG:  Status of job j-2403145d88cc41f0b4291d21141d96e7 is running (on backend cdse-staging).\n",
      "2024-03-14 11:51:15,494|openeo_gfmap.manager|DEBUG:  Status of job j-2403145d88cc41f0b4291d21141d96e7 is running (on backend cdse-staging).\n",
      "2024-03-14 11:52:15,811|openeo_gfmap.manager|DEBUG:  Status of job j-2403145d88cc41f0b4291d21141d96e7 is running (on backend cdse-staging).\n",
      "2024-03-14 11:53:16,172|openeo_gfmap.manager|DEBUG:  Status of job j-2403145d88cc41f0b4291d21141d96e7 is running (on backend cdse-staging).\n",
      "2024-03-14 11:54:16,549|openeo_gfmap.manager|DEBUG:  Status of job j-2403145d88cc41f0b4291d21141d96e7 is running (on backend cdse-staging).\n",
      "2024-03-14 11:55:17,278|openeo_gfmap.manager|DEBUG:  Status of job j-2403145d88cc41f0b4291d21141d96e7 is running (on backend cdse-staging).\n",
      "2024-03-14 11:56:17,654|openeo_gfmap.manager|DEBUG:  Status of job j-2403145d88cc41f0b4291d21141d96e7 is running (on backend cdse-staging).\n",
      "2024-03-14 11:57:18,481|openeo_gfmap.manager|DEBUG:  Status of job j-2403145d88cc41f0b4291d21141d96e7 is running (on backend cdse-staging).\n",
      "2024-03-14 11:58:19,099|openeo_gfmap.manager|DEBUG:  Status of job j-2403145d88cc41f0b4291d21141d96e7 is running (on backend cdse-staging).\n"
     ]
    }
   ],
   "source": [
    "# Run the jobs and create the STAC catalogue\n",
    "manager.run_jobs(job_df, create_datacube_s2, tracking_job_csv)\n",
    "manager.create_stac()  # By default the STAC catalogue will be saved in the stac subfolder of the base_output_dir folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfmap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
