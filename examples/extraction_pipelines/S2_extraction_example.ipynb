{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of a GFMAP full-extraction pipeline\n",
    "\n",
    "Designing a full-extraction pipeline using the openeo-gfmap GFMAPJobManager.\n",
    "\n",
    "The pipeline consits of the following elements:\n",
    "* An input dataframe where each row corresponds to each executed job.\n",
    "* An user function defined to create OpenEO BatchJob from input rows of the beforementioned dataframe.\n",
    "* An user function defined to generate an output path of each of the Job products\n",
    "* An user function executed after the assets are downloaded and saved from a finished job (optional). This **post job action** can do anything and will be executed locally inside the GFMAPJobManager.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the logging module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the logging for the openeo_gfmap package\n",
    "from openeo_gfmap.manager import _log\n",
    "import logging\n",
    "\n",
    "_log.setLevel(logging.DEBUG)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "_log.addHandler(stream_handler)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s|%(name)s|%(levelname)s:  %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "\n",
    "# Exclude the other loggers from other libraries\n",
    "class MyLoggerFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return record.name == _log.name\n",
    "\n",
    "stream_handler.addFilter(MyLoggerFilter())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step: splitting the job\n",
    "\n",
    "We load the initial crop-type dataset that will be the base of our extractions.\n",
    "\n",
    "Splitting the dataset of extraction in multiple job based on position is necessary to respect OpenEO limitations.\n",
    "\n",
    "This script performs a split with the H3 hexagonal grid, yielding a list of sub-geodataframes.\n",
    "\n",
    "A subtility here is that some polygons are not directly extracted (field with `extract=False`), but should be kept for post-job actions. This requirement is filled by removing sub-dataframes that do not contain any extractable polyons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "from openeo_gfmap.manager.job_splitters import split_job_s2grid, _append_h3_index\n",
    "\n",
    "base_df_path = \"https://artifactory.vgt.vito.be/artifactory/auxdata-public/gfmap/DEMO_CROPTYPE.gpkg\"\n",
    "base_df = gpd.read_file(base_df_path)\n",
    "base_df = _append_h3_index(base_df, grid_resolution=3)\n",
    "# Splits the job using GFMAP\n",
    "split_jobs = split_job_s2grid(\n",
    "    base_df, max_points=60\n",
    ")\n",
    "\n",
    "print(f'{len(split_jobs)} sub-datasets.')\n",
    "\n",
    "# Remove the geometry where there are no points with the \"extract\" flag\n",
    "split_jobs = [\n",
    "    job for job in split_jobs if job.extract.any()\n",
    "]\n",
    "print(f'{len(split_jobs)} sub-datasets after filtering sub-datasets with no point to extract.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second step: creating a dataframe for the GFMAP Job Manager\n",
    "\n",
    "Implementing a function that yields a `pandas.DataFrame` where each row correponds to a job.\n",
    "\n",
    "The dataframe should contain the informations required by the GFMAP Job Manager, as well as additional information used by the datacube creation function and the post-job action function.\n",
    "\n",
    "The output dataframe should be savable as a .csv file.\n",
    "\n",
    "Note: the full information of a sub-geodataframe of polygons can be saved into a row of a `pandas.DataFrame` by storing it in a row as string implementing the `geojson.FeatureCollection` interface. To convert the `geopandas.GeoDataFrame` into a stirng, simply use the `.to_json()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openeo_gfmap import Backend\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "def create_job_dataframe_s2(backend: Backend, split_jobs: List[gpd.GeoDataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"Create a dataframe from the split jobs, containg all the necessary information to run the job.\"\"\"\n",
    "    columns = ['backend_name', 'out_prefix', 'out_extension', 'start_date', 'end_date', 's2_tile', 'h3index', 'geometry']\n",
    "    rows = []\n",
    "    for job in split_jobs:\n",
    "        # Compute the average in the valid date and make a buffer of 1.5 year around\n",
    "        median_time = pd.to_datetime(job.valid_date).mean()\n",
    "        start_date = median_time - pd.Timedelta(days=275)  # A bit more than 9 months\n",
    "        end_date = median_time + pd.Timedelta(days=275)  # A bit more than 9 months\n",
    "        s2_tile = job.tile.iloc[0]  # Job dataframes are split depending on the\n",
    "        h3index = job.h3index.iloc[0] \n",
    "        \n",
    "        rows.append(\n",
    "            pd.Series(\n",
    "                dict(zip(columns, [backend.value, 'S2-L2A-10m', '.nc',  start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'), s2_tile, h3index, job.to_json()]))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "job_df = create_job_dataframe_s2(Backend.CDSE, split_jobs)\n",
    "\n",
    "job_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-sampling job dataframe to reduce execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a subset of the jobs to test the manager, the selected jobs have a fair amount of geometries to extract\n",
    "# 8, 6, 4, 2 and 1 tiles\n",
    "job_df = job_df.iloc[[0]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geojson\n",
    "\n",
    "def get_job_nb_polygons(row: pd.Series) -> int:\n",
    "    \"\"\"Get the number of polygons in the geometry.\"\"\"\n",
    "    return len(list(filter(lambda feat: feat.properties.get(\"extract\"), geojson.loads(row.geometry)['features'])))\n",
    "\n",
    "job_df['nb_polygons'] = job_df.apply(get_job_nb_polygons, axis=1)\n",
    "job_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third step: implement the datacube creator function.\n",
    "\n",
    "Implement a function to create, from the additional rows provided before, an `openeo.BatchJob` that will be used to run the job.\n",
    "\n",
    "In this case we extract Sentinel-2 data around a 64x64 pixel square of polygons which have the field `extract=True` (although we keep them in the row for the post-job action.)\n",
    "\n",
    "Note:\n",
    "Because the polygons to extract are specified in UTM dimensions (required to have a specific size), the dataset of polygon cannot be send directly through the openeo process graph (GeoJSON only support lat/lon coordinates). The sub-datasets of polygons are therefore uploaded to a publicly accessible URL so they can be used later by openeo during the execution of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openeo\n",
    "\n",
    "import requests\n",
    "from tempfile import NamedTemporaryFile\n",
    "import os\n",
    "import pandas as pd\n",
    "import geojson\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from openeo_gfmap import TemporalContext, Backend, BackendContext, FetchType, SpatialContext\n",
    "from openeo_gfmap.fetching import build_sentinel2_l2a_extractor\n",
    "\n",
    "def upload_geoparquet_artifactory(gdf: gpd.GeoDataFrame, row_id: int) -> str:\n",
    "    # Save the dataframe as geoparquet to upload it to artifactory\n",
    "    temporary_file = NamedTemporaryFile()\n",
    "    gdf.to_parquet(temporary_file.name)\n",
    "    \n",
    "    artifactory_username = os.getenv('ARTIFACTORY_USERNAME')\n",
    "    artifactory_password = os.getenv('ARTIFACTORY_PASSWORD')\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/octet-stream\"\n",
    "    }\n",
    "\n",
    "    upload_url = f\"https://artifactory.vgt.vito.be/artifactory/auxdata-public/gfmap-temp/openeogfmap_df_{row_id}.parquet\"\n",
    "\n",
    "    with open(temporary_file.name, 'rb') as f:\n",
    "        response = requests.put(upload_url, headers=headers, data=f, auth=(artifactory_username, artifactory_password))\n",
    "\n",
    "    assert response.status_code == 201, f\"Error uploading the dataframe to artifactory: {response.text}\"\n",
    "\n",
    "    return upload_url\n",
    "\n",
    "\n",
    "def create_datacube_s2(row: pd.Series, connection: openeo.DataCube, provider=None, connection_provider=None) -> openeo.BatchJob:\n",
    "\n",
    "    def buffer_geometry(geometry: geojson.FeatureCollection, buffer: int) -> gpd.GeoDataFrame:\n",
    "        gdf = gpd.GeoDataFrame.from_features(geometry).set_crs(epsg=4326)\n",
    "        utm = gdf.estimate_utm_crs()\n",
    "        gdf = gdf.to_crs(utm)\n",
    "\n",
    "        gdf['geometry'] = gdf.centroid.apply(\n",
    "            # Clips the point to the closest 20m from the S2 grid\n",
    "            lambda point: Point(round(point.x / 20.0) * 20.0, round(point.y / 20.0) * 20.0)\n",
    "        ).buffer(distance=buffer, cap_style=3)\n",
    "\n",
    "        return gdf\n",
    "\n",
    "    def filter_extractonly_geometries(collection: geojson.FeatureCollection):\n",
    "        # Filter out geometries that do not have the field extract=True\n",
    "        features = [f for f in collection.features if f.properties.get('extract', False)]\n",
    "        return geojson.FeatureCollection(features)\n",
    "\n",
    "    start_date = row.start_date\n",
    "    end_date = row.end_date\n",
    "    temporal_context = TemporalContext(start_date, end_date)\n",
    "\n",
    "    # Get the feature collection containing the geometry to the job\n",
    "    geometry = geojson.loads(row.geometry)\n",
    "    assert isinstance(geometry, geojson.FeatureCollection)\n",
    "\n",
    "    # Filter the geometry to the rows with the extract only flag\n",
    "    geometry = filter_extractonly_geometries(geometry)\n",
    "    assert len(geometry.features) > 0, \"No geometries with the extract flag found\"\n",
    "\n",
    "    # Performs a buffer of 64 px around the geometry\n",
    "    geometry_df = buffer_geometry(geometry, 320)\n",
    "    spatial_extent_url = upload_geoparquet_artifactory(geometry_df, row.name)\n",
    "\n",
    "    # Backend name and fetching type\n",
    "    backend = Backend(row.backend_name)\n",
    "    backend_context = BackendContext(backend)\n",
    "\n",
    "    fetch_type = FetchType.POLYGON\n",
    "    bands_to_download = ['S2-L2A-B01', 'S2-L2A-B02', 'S2-L2A-B03', 'S2-L2A-B04', 'S2-L2A-B05', 'S2-L2A-B06', 'S2-L2A-B07', 'S2-L2A-B08', 'S2-L2A-B8A', 'S2-L2A-B09', 'S2-L2A-B11', 'S2-L2A-B12', 'S2-L2A-SCL']\n",
    "\n",
    "    # Compute the SCL dilation and add it to the cube\n",
    "    sub_collection = connection.load_collection(\n",
    "        collection_id=\"SENTINEL2_L2A\",\n",
    "        bands=[\"SCL\"],\n",
    "        temporal_extent=[start_date, end_date],\n",
    "        properties={\n",
    "            \"eo:cloud_cover\": lambda val: val <= 95.0,\n",
    "            \"tileId\": lambda val: val == row.s2_tile\n",
    "        }\n",
    "    )\n",
    "    scl_dilated_mask = sub_collection.process(\n",
    "        \"to_scl_dilation_mask\",\n",
    "        data=sub_collection,\n",
    "        scl_band_name=\"SCL\",\n",
    "        kernel1_size=17,  # 17px dilation on a 20m layer\n",
    "        kernel2_size=77,   # 77px dilation on a 20m layer\n",
    "        mask1_values=[2, 4, 5, 6, 7],\n",
    "        mask2_values=[3, 8, 9, 10, 11],\n",
    "        erosion_kernel_size=3\n",
    "    ).rename_labels(\"bands\", [\"S2-L2A-SCL_DILATED_MASK\"])\n",
    "\n",
    "    # Create the job to extract S2\n",
    "    extraction_parameters = {\n",
    "        \"target_resolution\": 10,\n",
    "        \"load_collection\": {\n",
    "            \"eo:cloud_cover\": lambda val: val <= 95.0,\n",
    "            \"tileId\": lambda val: val == row.s2_tile\n",
    "        },\n",
    "        \"additional_masks\": scl_dilated_mask  # Add an additional mask computed from the SCL layer\n",
    "    }\n",
    "    extractor = build_sentinel2_l2a_extractor(\n",
    "        backend_context, bands=bands_to_download, fetch_type=fetch_type.POLYGON, **extraction_parameters \n",
    "    )\n",
    "\n",
    "    cube = extractor.get_cube(connection, spatial_extent_url, temporal_context)\n",
    "\n",
    "    # Compute the distance to cloud and add it to the cube\n",
    "    scl = cube.filter_bands(['S2-L2A-SCL'])\n",
    "    distance_to_cloud = scl.apply_neighborhood(\n",
    "        process=openeo.UDF.from_file('udf_distance_to_cloud.py'),\n",
    "        size=[{'dimension': 'x', 'unit': 'px', 'value': 256}, {'dimension': 'y', 'unit': 'px', 'value': 256}],\n",
    "        overlap=[{'dimension': 'x', 'unit': 'px', 'value': 16}, {'dimension': 'y', 'unit': 'px', 'value': 16}]\n",
    "    ).rename_labels('bands', ['S2-L2A-DISTANCE-TO-CLOUD'])\n",
    "\n",
    "    cube = cube.merge_cubes(distance_to_cloud)\n",
    "    cube = cube.linear_scale_range(0, 65534, 0, 65534)\n",
    "\n",
    "    # Get the h3index to use in the tile\n",
    "    s2_tile = row.s2_tile\n",
    "    valid_date = geometry.features[0].properties['valid_date']\n",
    "\n",
    "    # Increase the memory of the jobs depending on the number of polygons to extract\n",
    "    number_polygons = get_job_nb_polygons(row)\n",
    "    _log.debug(f\"Number of polygons to extract: {number_polygons}\")\n",
    "\n",
    "    job_options = {\n",
    "        \"executor-memory\": \"5G\",\n",
    "        \"executor-memoryOverhead\": \"2G\",\n",
    "    }\n",
    "\n",
    "    return cube.create_job(\n",
    "        out_format=\"NetCDF\",\n",
    "        title=f\"GFMAP_Extraction_S2_{s2_tile}_{valid_date}\",\n",
    "        sample_by_feature=True,\n",
    "        job_options=job_options,\n",
    "        feauture_id_property=\"sample_id\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth step: create output paths\n",
    "\n",
    "Implement a function that from the sample index the job row determines which path to saves the assets to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openeo_gfmap.manager.job_splitters import _load_s2_grid\n",
    "\n",
    "# Load the S2 grid\n",
    "s2_grid = _load_s2_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "\n",
    "def generate_output_path_s2(root_folder: Path, geometry_index: int, row: pd.Series):\n",
    "    features = geojson.loads(row.geometry)\n",
    "    sample_id = features[geometry_index].properties['sample_id']\n",
    "    ref_id = features[geometry_index].properties['ref_id']\n",
    "    \n",
    "    s2_tile_id = row.s2_tile\n",
    "    h3index = row.h3index\n",
    "    epsg = s2_grid[s2_grid.tile == s2_tile_id].iloc[0].epsg\n",
    "    \n",
    "    subfolder = root_folder / ref_id / h3index / sample_id\n",
    "    return subfolder / f'{row.out_prefix}_{sample_id}_{epsg}_{row.start_date}_{row.end_date}{row.out_extension}'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fifth step: Define the post-job action\n",
    "\n",
    "The post-job action will be called once the job resut was downloaded and saved to a specific path.\n",
    "\n",
    "A post-job action function must receive 3 parameters:\n",
    "* `job_items`: STAC items containing the currently extracted data for the job.\n",
    "* `row`: The current job dataframe row.\n",
    "* `parameters`: User-defined parameters set in the `GFMAPJobManager` constructor.\n",
    "\n",
    "The post-job action must return back the list of job items. The user is responsible for updating,\n",
    "adding and removing the items. For example, in this case the user creates a raster file with\n",
    "ground truth data, the user then adds an asset using the predefined `openeo_gfmap.stac.AUXILIARY`\n",
    "AssetDefintion to the related item, pointing to the generated NetCDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from importlib.metadata import version\n",
    "from datetime import datetime\n",
    "\n",
    "import pystac\n",
    "from pyproj import CRS\n",
    "from shapely.geometry import box\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.transform import from_bounds\n",
    "# from openeo_gfmap.stac import AUXILIARY\n",
    "\n",
    "# Define auxiliary asset:\n",
    "auxiliary_asset = pystac.extensions.item_assets.AssetDefinition(\n",
    "    {\n",
    "        \"title\": \"ground truth data\",\n",
    "        \"description\": \"This asset contains the crop type codes.\",\n",
    "        \"type\": \"application/x-netcdf\",\n",
    "        \"roles\": [\"data\"],\n",
    "        \"proj:shape\": [64, 64],\n",
    "        \"raster:bands\": [{\"name\": \"CROPTYPE\", \"data_type\": \"uint16\", \"bits_per_sample\": 16}],\n",
    "    }\n",
    ")\n",
    "\n",
    "def add_item_asset(related_item: pystac.Item, path: Path):\n",
    "    asset = auxiliary_asset.create_asset(\n",
    "        href=path.as_posix()\n",
    "    )\n",
    "    related_item.add_asset('auxiliary', asset)\n",
    "\n",
    "def post_job_action(job_items: List[pystac.Item], row: pd.Series, parameters: dict = {}) -> list:\n",
    "    base_gpd = gpd.GeoDataFrame.from_features(json.loads(row.geometry)).set_crs(epsg=4326)\n",
    "    assert len(base_gpd[base_gpd.extract == True]) == len(job_items), \"The number of result paths should be the same as the number of geometries\"\n",
    "    extracted_gpd = base_gpd[base_gpd.extract == True].reset_index(drop=True)\n",
    "    # In this case we want to burn the metadata in a new file in the same folder as the S2 product\n",
    "    for idx, item in enumerate(job_items):\n",
    "        sample_id = extracted_gpd.iloc[idx].sample_id\n",
    "        ref_id = extracted_gpd.iloc[idx].ref_id\n",
    "        confidence = extracted_gpd.iloc[idx].confidence\n",
    "        valid_date = extracted_gpd.iloc[idx].valid_date\n",
    "\n",
    "        item_asset_path = Path(\n",
    "            list(item.assets.values())[0].href\n",
    "        )\n",
    "        # Read information from the item file (could also read it from the item object metadata)\n",
    "        result_ds = xr.open_dataset(item_asset_path)\n",
    "        \n",
    "        # Add some metadata to the result_df netcdf file\n",
    "        result_ds.attrs.update({\n",
    "            'start_date': row.start_date,\n",
    "            'end_date': row.end_date,\n",
    "            'valid_date': valid_date,\n",
    "            'GFMAP_version': version('openeo_gfmap'),\n",
    "            'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'description': f'Sentinel2 L2A observations for sample: {sample_id}, unprocessed.',\n",
    "            'title': f'Sentinel2 L2A - {sample_id}',\n",
    "            'sample_id': sample_id,\n",
    "            'spatial_resolution': '10m'\n",
    "        })\n",
    "        \n",
    "        result_ds.to_netcdf(item_asset_path)\n",
    "        \n",
    "        target_crs = CRS.from_wkt(result_ds.crs.attrs['crs_wkt'])\n",
    "\n",
    "        # Get the surrounding polygons around our extracted center geometry to rastetize them\n",
    "        bounds = (\n",
    "            result_ds.x.min().item(),\n",
    "            result_ds.y.min().item(),\n",
    "            result_ds.x.max().item(),\n",
    "            result_ds.y.max().item()\n",
    "        )\n",
    "        bbox = box(*bounds)\n",
    "        surround_gpd = base_gpd.to_crs(target_crs).clip(bbox)\n",
    "\n",
    "        # Burn the polygon croptypes\n",
    "        transform = from_bounds(*bounds, result_ds.x.size, result_ds.y.size)\n",
    "        croptype_shapes = list(zip(surround_gpd.geometry, surround_gpd.croptype_label))\n",
    "\n",
    "        fill_value = 0\n",
    "        croptype = rasterize(\n",
    "            croptype_shapes,\n",
    "            out_shape=(result_ds.y.size, result_ds.x.size),\n",
    "            transform=transform,\n",
    "            all_touched=False,\n",
    "            fill=fill_value,\n",
    "            default_value=0,\n",
    "            dtype='int64'\n",
    "        )\n",
    "\n",
    "        # Create the attributes to add to the metadata\n",
    "        attributes = {\n",
    "            'ref_id': ref_id,\n",
    "            'sample_id': sample_id,\n",
    "            'confidence': str(confidence),\n",
    "            'valid_date': valid_date,\n",
    "            '_FillValue': fill_value,\n",
    "            'Conventions': 'CF-1.9',\n",
    "            'GFMAP_version': version('openeo_gfmap'),\n",
    "            'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'description': f'Contains rasterized WorldCereal labels for sample: {sample_id}.',\n",
    "            'title': f'WORLDCEREAL Auxiliary file for sample: {sample_id}',\n",
    "            'spatial_resolution': '10m'\n",
    "        }\n",
    "\n",
    "        aux_dataset = xr.Dataset(\n",
    "            {'LABEL': (('y', 'x'), croptype), 'crs': result_ds['crs']},\n",
    "            coords={'y': result_ds.y, 'x': result_ds.x},\n",
    "            attrs=attributes\n",
    "        )\n",
    "        # Required to map the 'crs' layer as geo-reference for the 'LABEL' layer.\n",
    "        aux_dataset['LABEL'].attrs['grid_mapping'] = 'crs'\n",
    "\n",
    "        # Save the metadata in the same folder as the S2 product\n",
    "        metadata_path = (\n",
    "            item_asset_path.parent / \n",
    "            f'WORLDCEREAL_10m_{sample_id}_{target_crs.to_epsg()}_{valid_date}.nc'\n",
    "        )\n",
    "\n",
    "        aux_dataset.to_netcdf(metadata_path, format='NETCDF4', engine='h5netcdf')\n",
    "        aux_dataset.close()\n",
    "\n",
    "        # Adds this metadata as a new asset\n",
    "        add_item_asset(item, metadata_path)\n",
    "        \n",
    "    return job_items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sixth and last step: Running the manager\n",
    "\n",
    "Let's initialize and execute the Job Manager as defined the GFMAP, and then run it using the functions defined previously\n",
    "\n",
    "STAC related parameters such as `collection_id` and `collection_description` are also required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from openeo_gfmap.manager.job_manager import GFMAPJobManager\n",
    "from openeo_gfmap.backend import cdse_staging_connection\n",
    "\n",
    "\n",
    "base_output_dir = Path('/data/users/Public/couchard/world_cereal/extractions_bys2tile_4/')\n",
    "tracking_job_csv = base_output_dir / 'job_tracker.csv'\n",
    "\n",
    "manager = GFMAPJobManager(\n",
    "    output_dir=base_output_dir,\n",
    "    output_path_generator=generate_output_path_s2,\n",
    "    collection_id='SENTINEL2-EXTRACTION',\n",
    "    collection_description=(\n",
    "        \"Sentinel-2 and Auxiliary data extraction example.\"\n",
    "    ),\n",
    "    post_job_action=post_job_action,\n",
    "    poll_sleep=60,\n",
    "    n_threads=2,\n",
    "    post_job_params={}\n",
    ")\n",
    "\n",
    "manager.add_backend(\n",
    "    Backend.CDSE_STAGING.value, cdse_staging_connection, parallel_jobs=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 11:32:22,783|openeo_gfmap.manager|INFO:  Starting ThreadPoolExecutor with 2 workers.\n",
      "2024-03-20 11:32:22,784|openeo_gfmap.manager|INFO:  Creating and running jobs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 11:32:22,790|openeo_gfmap.manager|DEBUG:  Normalizing dataframe. Columns: Index(['backend_name', 'out_prefix', 'out_extension', 'start_date', 'end_date',\n",
      "       's2_tile', 'h3index', 'geometry', 'nb_polygons', 'status', 'id',\n",
      "       'start_time', 'cpu', 'memory', 'duration', 'description', 'costs'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated using refresh token.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 11:32:24,137|openeo_gfmap.manager|DEBUG:  Status of job j-24032058198e49aebbde5f6041ffa3d7 is finished (on backend cdse-staging).\n",
      "2024-03-20 11:32:24,138|openeo_gfmap.manager|INFO:  Job j-24032058198e49aebbde5f6041ffa3d7 finished successfully, queueing on_job_done...\n",
      "2024-03-20 11:32:24,662|openeo_gfmap.manager|DEBUG:  Generating output path for asset openEO_0.nc from job j-24032058198e49aebbde5f6041ffa3d7...\n",
      "2024-03-20 11:32:28,082|openeo_gfmap.manager|DEBUG:  Downloaded openEO_0.nc from job j-24032058198e49aebbde5f6041ffa3d7 -> /data/users/Public/couchard/world_cereal/extractions_bys2tile_4/2021_EUR_DEMO_POLY_110/83194dfffffffff/2021_BE_Flanders_full_2195082011/S2-L2A-10m_2021_BE_Flanders_full_2195082011_32631_2020-08-30_2022-03-03.nc\n",
      "2024-03-20 11:32:29,939|openeo_gfmap.manager|INFO:  Parsed item j-24032058198e49aebbde5f6041ffa3d7_openEO_0.nc from job j-24032058198e49aebbde5f6041ffa3d7\n",
      "2024-03-20 11:32:29,941|openeo_gfmap.manager|DEBUG:  Calling post job action for job j-24032058198e49aebbde5f6041ffa3d7...\n",
      "2024-03-20 11:32:30,806|openeo_gfmap.manager|INFO:  Added 1 items to the STAC collection.\n",
      "2024-03-20 11:32:30,807|openeo_gfmap.manager|INFO:  Job j-24032058198e49aebbde5f6041ffa3d7 and post job action finished successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define auxiliary asset:\n",
    "import pystac\n",
    "auxiliary_asset = pystac.extensions.item_assets.AssetDefinition(\n",
    "    {\n",
    "        \"title\": \"ground truth data\",\n",
    "        \"description\": \"This asset contains the crop type codes.\",\n",
    "        \"type\": \"application/x-netcdf\",\n",
    "        \"roles\": [\"data\"],\n",
    "        \"proj:shape\": [64, 64],\n",
    "        \"raster:bands\": [{\"name\": \"CROPTYPE\", \"data_type\": \"uint16\", \"bits_per_sample\": 16}],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Run the jobs and create the STAC catalogue\n",
    "manager.run_jobs(job_df, create_datacube_s2, tracking_job_csv)\n",
    "# In the post job actions, an extra 'auxiliary' asset was added to each item. The asset definition is therefore also added to the item_asset_definitions extensions of the final STAC collection\n",
    "manager.create_stac(asset_definitions={'auxiliary': auxiliary_asset})  # By default the STAC catalogue will be saved in the stac subfolder of the base_output_dir folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openeo-dev",
   "language": "python",
   "name": "openeo-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
