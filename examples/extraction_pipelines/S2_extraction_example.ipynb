{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of a GFMAP full-extraction pipeline\n",
    "\n",
    "Designing of GFMAP Job DataFrames and DataCube creators functions, as well as post job-actions.\n",
    "\n",
    "Those dataframe should be containing all the necessary infromation to run a job and know where to save it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step: splitting the job\n",
    "\n",
    "Splitting the dataset of extraction in multiple job based on position is necessary to respect OpenEO limitations.\n",
    "\n",
    "This script performs a split with the H3 hexagonal grid, yielding a list of sub-geodataframes.\n",
    "\n",
    "A subtility here is that some polygons are not directly extracted (field with `extract=False`), but should be kept for post-job actions. This requirement is filled by removing sub-dataframes that do not contain any extractable polyons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the logging for the openeo_gfmap package\n",
    "from openeo_gfmap.manager import _log\n",
    "import logging\n",
    "\n",
    "_log.setLevel(logging.DEBUG)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "_log.addHandler(stream_handler)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s|%(name)s|%(levelname)s:  %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "\n",
    "# Exclude the other loggers from other libraries\n",
    "class MyLoggerFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return record.name == _log.name\n",
    "\n",
    "stream_handler.addFilter(MyLoggerFilter())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 jobs before filtering empty one (no extraction)\n",
      "93 jobs after filtering empty one (no extraction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vverelst/openeo/openeo-gfmap/src/openeo_gfmap/manager/job_splitters.py:53: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  polygons[\"h3index\"] = polygons.geometry.centroid.apply(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "from openeo_gfmap.manager.job_splitters import split_job_hex\n",
    "\n",
    "base_df_path = Path('./2021_EUR_DEMO_POLY_110.gpkg')\n",
    "base_df = gpd.read_file(base_df_path)\n",
    "# Splits the job using GFMAP\n",
    "split_jobs = split_job_hex(\n",
    "    base_df, max_points=500, grid_resolution=4\n",
    ")\n",
    "\n",
    "print(f'{len(split_jobs)} jobs before filtering empty one (no extraction)')\n",
    "\n",
    "# Remove the geometry where there are no points with the \"extract\" flag\n",
    "split_jobs = [\n",
    "    job for job in split_jobs if job.extract.any()\n",
    "]\n",
    "print(f'{len(split_jobs)} jobs after filtering empty one (no extraction)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second step: creating a dataframe for the GFMAP Job Manager\n",
    "\n",
    "Implementing a function that yields a `pandas.DataFrame` where each row correponds to a job.\n",
    "\n",
    "The dataframe should contain the informations required by the GFMAP Job Manager, as well as additional information used by the datacube creation function and the post-job action function.\n",
    "\n",
    "The output dataframe should be savable as a .csv file.\n",
    "\n",
    "Note: the full information of a sub-geodataframe of polygons can be saved into a row of a `pandas.DataFrame` by storing it in a row as string implementing the `geojson.FeatureCollection` interface. To convert the `geopandas.GeoDataFrame` into a stirng, simply use the `.to_json()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backend_name</th>\n",
       "      <th>out_prefix</th>\n",
       "      <th>out_extension</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   backend_name out_prefix out_extension  start_date    end_date  \\\n",
       "0          cdse         S2           .nc  2020-08-30  2022-03-03   \n",
       "1          cdse         S2           .nc  2020-08-30  2022-03-03   \n",
       "2          cdse         S2           .nc  2020-08-30  2022-03-03   \n",
       "3          cdse         S2           .nc  2020-08-30  2022-03-03   \n",
       "4          cdse         S2           .nc  2020-08-30  2022-03-03   \n",
       "..          ...        ...           ...         ...         ...   \n",
       "88         cdse         S2           .nc  2020-08-30  2022-03-03   \n",
       "89         cdse         S2           .nc  2020-08-30  2022-03-03   \n",
       "90         cdse         S2           .nc  2020-08-30  2022-03-03   \n",
       "91         cdse         S2           .nc  2020-08-30  2022-03-03   \n",
       "92         cdse         S2           .nc  2020-08-30  2022-03-03   \n",
       "\n",
       "                                             geometry  \n",
       "0   {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "1   {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "2   {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "3   {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "4   {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "..                                                ...  \n",
       "88  {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "89  {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "90  {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "91  {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "92  {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "\n",
       "[93 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openeo_gfmap import Backend\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "def create_job_dataframe_s2(backend: Backend, split_jobs: List[gpd.GeoDataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"Create a dataframe from the split jobs, containg all the necessary information to run the job.\"\"\"\n",
    "    columns = ['backend_name', 'out_prefix', 'out_extension', 'start_date', 'end_date', 'geometry']\n",
    "    rows = []\n",
    "    for job in split_jobs:\n",
    "        # Compute the average in the valid date and make a buffer of 1.5 year around\n",
    "        median_time = pd.to_datetime(job.valid_date).mean()\n",
    "        start_date = median_time - pd.Timedelta(days=275)  # A bit more than 9 months\n",
    "        end_date = median_time + pd.Timedelta(days=275)  # A bit more than 9 months\n",
    "        \n",
    "        rows.append(\n",
    "            pd.Series(\n",
    "                dict(zip(columns, [backend.value, 'S2', '.nc',  start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'), job.to_json()]))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "job_df = create_job_dataframe_s2(Backend.CDSE, split_jobs)\n",
    "\n",
    "job_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backend_name</th>\n",
       "      <th>out_prefix</th>\n",
       "      <th>out_extension</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  backend_name out_prefix out_extension  start_date    end_date  \\\n",
       "0         cdse         S2           .nc  2020-08-30  2022-03-03   \n",
       "\n",
       "                                            geometry  \n",
       "0  {\"type\": \"FeatureCollection\", \"features\": [{\"i...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run a subset of the jobs to test the manager, the selected jobs have a fair amount of geometries to extract\n",
    "job_df = job_df.iloc[[0]].reset_index(drop=True)\n",
    "job_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third step: implement the datacube creator function.\n",
    "\n",
    "Implement a function to create, from the additional rows provided before, an `openeo.BatchJob` that will be used to run the job.\n",
    "\n",
    "In this case we extract Sentinel-2 data, and we remove the polygons with `extract=False` (although we keep them in the row for the post-job action.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openeo\n",
    "\n",
    "import pandas as pd\n",
    "import geojson\n",
    "\n",
    "from openeo_gfmap import TemporalContext, Backend, BackendContext, FetchType\n",
    "from openeo_gfmap.fetching import build_sentinel2_l2a_extractor\n",
    "\n",
    "\n",
    "def create_datacube_s2(row: pd.Series, connection: openeo.DataCube, provider=None, connection_provider=None) -> openeo.BatchJob:\n",
    "\n",
    "    def buffer_geometry(geometry: geojson.FeatureCollection, buffer: float) -> str:\n",
    "        gdf = gpd.GeoDataFrame.from_features(geometry).set_crs(epsg=4326)\n",
    "        utm = gdf.estimate_utm_crs()\n",
    "        gdf = gdf.to_crs(utm)\n",
    "        gdf['geometry'] = gdf.centroid.buffer(distance=buffer, cap_style=3)\n",
    "\n",
    "        gdf = gdf.to_crs(epsg=4326)\n",
    "        return geojson.loads(gdf.to_json())\n",
    "\n",
    "\n",
    "    def filter_extractonly_geometries(collection: geojson.FeatureCollection):\n",
    "        # Filter out geometries that do not have the field extract=True\n",
    "        features = [f for f in collection.features if f.properties.get('extract', False)]\n",
    "        return geojson.FeatureCollection(features)\n",
    "\n",
    "    start_date = row.start_date\n",
    "    end_date = row.end_date\n",
    "    temporal_context = TemporalContext(start_date, end_date)\n",
    "\n",
    "    # Get the feature collection containing the geometry to the job\n",
    "    geometry = geojson.loads(row.geometry)\n",
    "    assert isinstance(geometry, geojson.FeatureCollection)\n",
    "\n",
    "    # Filter the geometry to the rows with the extract only flag\n",
    "    geometry = filter_extractonly_geometries(geometry)\n",
    "    assert len(geometry.features) > 0, \"No geometries with the extract flag found\"\n",
    "\n",
    "    # Performs a buffer of 64 px around the geometry\n",
    "    geometry = buffer_geometry(geometry, 319)\n",
    "\n",
    "    # Backend name and fetching type\n",
    "    backend = Backend(row.backend_name)\n",
    "    backend_context = BackendContext(backend)\n",
    "\n",
    "    fetch_type = FetchType.POLYGON\n",
    "    bands_to_download = ['S2-B01', 'S2-B02', 'S2-B03', 'S2-B04', 'S2-B05', 'S2-B06', 'S2-B07', 'S2-B08', 'S2-B8A', 'S2-B09', 'S2-B11', 'S2-B12', 'S2-SCL']\n",
    "\n",
    "    # Create the job to extract S2\n",
    "    extraction_parameters = {\n",
    "        \"target_resolution\": 10\n",
    "    }\n",
    "    extractor = build_sentinel2_l2a_extractor(\n",
    "        backend_context, bands=bands_to_download, fetch_type=fetch_type.POLYGON, **extraction_parameters \n",
    "    )\n",
    "\n",
    "    cube = extractor.get_cube(connection, geometry, temporal_context)\n",
    "\n",
    "    # Get the h3index to use in the tile\n",
    "    h3index = geometry.features[0].properties['h3index']\n",
    "    valid_date = geometry.features[0].properties['valid_date']\n",
    "\n",
    "    # Increase the memory of the jobs\n",
    "    job_options = {\n",
    "        \"executor-memory\": \"5G\",\n",
    "        \"executor-memoryOverhead\": \"2G\",\n",
    "    }\n",
    "\n",
    "    return cube.create_job(\n",
    "        out_format=\"NetCDF\",\n",
    "        title=f\"GFMAP_Extraction_S2_{h3index}_{valid_date}\",\n",
    "        sample_by_feature=True,\n",
    "        job_options=job_options\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth step: create output paths\n",
    "\n",
    "Implement a function that from a temporary path containing a job result, from the job dataframe row and the root folder will choose the output path where to save that job result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.23 s, sys: 23.9 ms, total: 2.25 s\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load the S2 grid\n",
    "s2_grid = gpd.read_file('./s2grid_bounds.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "from pyproj import Transformer, CRS\n",
    "from shapely.geometry import box, Point\n",
    "\n",
    "def generate_output_path_s2(root_folder: Path, tmp_path: Path, geometry_index: int, row: pd.Series):\n",
    "    features = geojson.loads(row.geometry)\n",
    "    sample_id = features[geometry_index].properties['sample_id']\n",
    "    ref_id = features[geometry_index].properties['ref_id']\n",
    "    \n",
    "    # Loads the array lazily in-memory\n",
    "    try:\n",
    "        inds = xr.open_dataset(tmp_path, chunks='auto')\n",
    "        \n",
    "        source_crs = CRS.from_wkt(inds.crs.attrs['crs_wkt'])\n",
    "        dst_crs = CRS.from_epsg(4326)\n",
    "        \n",
    "        transformer = Transformer.from_crs(source_crs, dst_crs, always_xy=True)\n",
    "        bounds = inds.x.min().item(), inds.y.min().item(), inds.x.max().item(), inds.y.max().item()\n",
    "        bbox = box(*bounds)\n",
    "\n",
    "        # Get the center of the box\n",
    "        centroid = bbox.centroid\n",
    "        lon, lat = transformer.transform(centroid.x, centroid.y)\n",
    "        centroid_pt = Point(lon, lat)\n",
    "\n",
    "        # Intersecting with the s2 grid\n",
    "        intersecting = s2_grid.geometry.intersects(centroid_pt)\n",
    "\n",
    "        # Select the intersecting cell that has a centroid the closest from the point\n",
    "        intersecting_cells = s2_grid[intersecting]\n",
    "        intersecting_cells['distance'] = intersecting_cells.distance(centroid_pt)\n",
    "        intersecting_cells.sort_values('distance', inplace=True)\n",
    "        s2_tile = intersecting_cells.iloc[0]\n",
    "\n",
    "        s2_tile_id = s2_tile.tile\n",
    "\n",
    "        subfolder = root_folder / ref_id / str(source_crs.to_epsg()) / s2_tile_id / sample_id\n",
    "    except Exception:\n",
    "        # TODO: _log.error('Could not find S2 tile for file, setting up a dummy path')\n",
    "        subfolder = root_folder / 'unsortable'\n",
    "\n",
    "    return subfolder / f'{row.out_prefix}_{sample_id}_{source_crs.to_epsg()}_{row.start_date}_{row.end_date}{row.out_extension}'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fifth step: Define the post-job action\n",
    "\n",
    "The post-job action will be called once the job resut was downloaded and saved to a specific path.\n",
    "\n",
    "A post-job action function must receive 3 parameters:\n",
    "* `result_paths`: Paths to the downloaded job result files.\n",
    "* `row`: The current job dataframe row.\n",
    "* `parameters`: User-defined parameters set in the `GFMAPJobManager` constructor.\n",
    "\n",
    "The post-job action must return a list of paths containing the results from that job. For example, if no file is created/deleted in the post-job action, then the user can simply return the list of paths it has received as input `result_paths`. If instead files are added or removed, then the user will need to modify this list accordingly before returning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.features import rasterize\n",
    "from rasterio.transform import from_bounds\n",
    "import json\n",
    "\n",
    "def post_job_action(result_paths: dict, row: pd.Series, parameters: dict = {}) -> list:\n",
    "    base_gpd = gpd.GeoDataFrame.from_features(json.loads(row.geometry)).set_crs(epsg=4326)\n",
    "    assert len(base_gpd[base_gpd.extract == True]) == len(result_paths), \"The number of result paths should be the same as the number of geometries\"\n",
    "    extracted_gpd = base_gpd[base_gpd.extract == True].reset_index(drop=True)\n",
    "    # In this case we want to burn the metadata in a new file in the same folder as the S2 product\n",
    "    for idx, result_path in enumerate(result_paths.copy()):\n",
    "        sample_id = extracted_gpd.iloc[idx].sample_id\n",
    "        ref_id = extracted_gpd.iloc[idx].ref_id\n",
    "        lc_label = extracted_gpd.iloc[idx].landcover_label\n",
    "        confidence = extracted_gpd.iloc[idx].confidence\n",
    "        valid_date = extracted_gpd.iloc[idx].valid_date\n",
    "\n",
    "        result_ds = xr.open_dataset(result_path, chunks='auto')\n",
    "\n",
    "        target_crs = CRS.from_wkt(result_ds.crs.attrs['crs_wkt'])\n",
    "\n",
    "        # Get the surrounding polygons around our extracted center geometry to rastetize them\n",
    "        bounds = (result_ds.x.min().item(), result_ds.y.min().item(), result_ds.x.max().item(), result_ds.y.max().item())\n",
    "        bbox = box(*bounds)\n",
    "        surround_gpd = base_gpd.to_crs(target_crs).clip(bbox)\n",
    "\n",
    "        # Burn the polygon croptypes\n",
    "        transform = from_bounds(*bounds, result_ds.x.size, result_ds.y.size)\n",
    "        croptype_shapes = list(zip(surround_gpd.geometry, surround_gpd.croptype_label))\n",
    "        croptype = rasterize(croptype_shapes, out_shape=(result_ds.y.size, result_ds.x.size), transform=transform, all_touched=True, fill=0, default_value=65535, dtype='uint16')\n",
    "\n",
    "        # Create the attributes to add to the metadata\n",
    "        crs_layer = result_ds['crs']\n",
    "        attributes = {\n",
    "            'ref_id': ref_id,\n",
    "            'sample_id': sample_id,\n",
    "            'landcover_label': lc_label,\n",
    "            'confidence': str(confidence),\n",
    "            'valid_date': valid_date\n",
    "        }\n",
    "        attributes.update(result_ds.attrs)\n",
    "\n",
    "        aux_dataset = xr.Dataset({'CROPTYPE': (('y', 'x'), croptype)}, coords={'y': result_ds.y, 'x': result_ds.x}, attrs=attributes)\n",
    "\n",
    "        # Include the CRS layer from OpenEO\n",
    "        aux_dataset['crs'] = crs_layer\n",
    "        aux_dataset.attrs.update(result_ds.attrs)\n",
    "\n",
    "        # Save the metadata in the same folder as the S2 product\n",
    "        metadata_path = result_path.parent / f'AUX_{sample_id}_{target_crs.to_epsg()}_{valid_date}.nc'\n",
    "        aux_dataset.to_netcdf(metadata_path, format='NETCDF4', engine='h5netcdf')\n",
    "        result_paths.append(metadata_path)\n",
    "\n",
    "    return result_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sixth and last step: Running the manager\n",
    "\n",
    "Let's initialize and execute the Job Manager as defined the GFMAP, and then run it using the functions defined previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openeo_gfmap.manager.job_manager import GFMAPJobManager\n",
    "from openeo_gfmap.backend import cdse_staging_connection\n",
    "\n",
    "base_output_dir = Path('/data/users/Public/vincent.verelst/world_cereal/extractions/')\n",
    "tracking_job_csv = base_output_dir / 'job_tracker.csv'\n",
    "\n",
    "manager = GFMAPJobManager(\n",
    "    output_dir=base_output_dir,\n",
    "    output_path_generator=generate_output_path_s2,\n",
    "    # post_job_action=post_job_action,\n",
    "    poll_sleep=60,\n",
    "    n_threads=2,\n",
    "    post_job_params={}\n",
    ")\n",
    "\n",
    "manager.add_backend(\n",
    "    Backend.CDSE_STAGING.value, cdse_staging_connection, parallel_jobs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 15:32:37,095|openeo_gfmap.manager|INFO:  Starting job manager using 2 worker threads.\n",
      "2024-02-22 15:32:37,098|openeo_gfmap.manager|INFO:  Workers started, creating and running jobs.\n",
      "2024-02-22 15:32:37,342|openeo_gfmap.manager|DEBUG:  Normalizing dataframe. Columns: Index(['backend_name', 'out_prefix', 'out_extension', 'start_date', 'end_date',\n",
      "       'geometry', 'status', 'id', 'start_time', 'cpu', 'memory', 'duration',\n",
      "       'description', 'costs'],\n",
      "      dtype='object')\n",
      "2024-02-22 15:32:37,346|openeo_gfmap.manager|DEBUG:  Updating status. 0 on 1 active jobs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated using refresh token.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_datacube_s2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracking_job_csv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/openeo/openeo-gfmap/src/openeo_gfmap/manager/job_manager.py:288\u001b[0m, in \u001b[0;36mGFMAPJobManager.run_jobs\u001b[0;34m(self, df, start_job, output_file)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads\u001b[38;5;241m.\u001b[39mappend(thread)\n\u001b[1;32m    287\u001b[0m _log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorkers started, creating and running jobs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 288\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_job\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/openeo/openeo-python-client/openeo/extra/job_management.py:296\u001b[0m, in \u001b[0;36mMultiBackendJobManager.run_jobs\u001b[0;34m(self, df, start_job, output_file)\u001b[0m\n\u001b[1;32m    294\u001b[0m             to_launch \u001b[38;5;241m=\u001b[39m df[df\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot_started\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m:to_add]\n\u001b[1;32m    295\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m to_launch\u001b[38;5;241m.\u001b[39mindex:\n\u001b[0;32m--> 296\u001b[0m                 \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_job\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persists(df, output_file)\n\u001b[1;32m    299\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_sleep)\n",
      "File \u001b[0;32m~/openeo/openeo-python-client/openeo/extra/job_management.py:331\u001b[0m, in \u001b[0;36mMultiBackendJobManager._launch_job\u001b[0;34m(self, start_job, df, i, backend_name)\u001b[0m\n\u001b[1;32m    328\u001b[0m     _log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting job on backend \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;241m.\u001b[39mto_dict()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    329\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection(backend_name, resilient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 331\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[43mstart_job\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnection_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    338\u001b[0m     _log\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to start job for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;241m.\u001b[39mto_dict()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[5], line 53\u001b[0m, in \u001b[0;36mcreate_datacube_s2\u001b[0;34m(row, connection, provider, connection_provider)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Create the job to extract S2\u001b[39;00m\n\u001b[1;32m     50\u001b[0m extraction_parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_resolution\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     52\u001b[0m }\n\u001b[0;32m---> 53\u001b[0m extractor \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_sentinel2_l2a_extractor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbands_to_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfetch_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOLYGON\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextraction_parameters\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m cube \u001b[38;5;241m=\u001b[39m extractor\u001b[38;5;241m.\u001b[39mget_cube(connection, geometry, temporal_context)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Get the h3index to use in the tile\u001b[39;00m\n",
      "File \u001b[0;32m~/openeo/openeo-gfmap/src/openeo_gfmap/fetching/s2.py:207\u001b[0m, in \u001b[0;36mbuild_sentinel2_l2a_extractor\u001b[0;34m(backend_context, bands, fetch_type, **params)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a S2 L2A extractor adapted to the given backend.\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m backend_functions \u001b[38;5;241m=\u001b[39m SENTINEL2_L2A_BACKEND_MAP\u001b[38;5;241m.\u001b[39mget(backend_context\u001b[38;5;241m.\u001b[39mbackend)\n\u001b[1;32m    206\u001b[0m fetcher, preprocessor \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 207\u001b[0m     \u001b[43mbackend_functions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfetch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m(fetch_type\u001b[38;5;241m=\u001b[39mfetch_type),\n\u001b[1;32m    208\u001b[0m     backend_functions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m](fetch_type\u001b[38;5;241m=\u001b[39mfetch_type),\n\u001b[1;32m    209\u001b[0m )\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CollectionFetcher(backend_context, bands, fetcher, preprocessor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "manager.run_jobs(job_df, create_datacube_s2, tracking_job_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the STAC collection to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.create_stac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openeo-dev",
   "language": "python",
   "name": "openeo-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
