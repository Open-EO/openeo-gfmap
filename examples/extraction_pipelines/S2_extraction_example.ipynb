{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of a GFMAP full-extraction pipeline\n",
    "\n",
    "Designing a full-extraction pipeline using the openeo-gfmap GFMAPJobManager.\n",
    "\n",
    "The pipeline consits of the following elements:\n",
    "* An input dataframe where each row corresponds to each executed job.\n",
    "* An user function defined to create OpenEO BatchJob from input rows of the beforementioned dataframe.\n",
    "* An user function defined to generate an output path of each of the Job products\n",
    "* An user function executed after the assets are downloaded and saved from a finished job (optional). This **post job action** can do anything and will be executed locally inside the GFMAPJobManager.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the logging module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the logging for the openeo_gfmap package\n",
    "from openeo_gfmap.manager import _log\n",
    "import logging\n",
    "\n",
    "_log.setLevel(logging.DEBUG)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "_log.addHandler(stream_handler)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s|%(name)s|%(levelname)s:  %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "\n",
    "# Exclude the other loggers from other libraries\n",
    "class MyLoggerFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return record.name == _log.name\n",
    "\n",
    "stream_handler.addFilter(MyLoggerFilter())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step: splitting the job\n",
    "\n",
    "We load the initial crop-type dataset that will be the base of our extractions.\n",
    "\n",
    "Splitting the dataset of extraction in multiple job based on position is necessary to respect OpenEO limitations.\n",
    "\n",
    "This script performs a split with the H3 hexagonal grid, yielding a list of sub-geodataframes.\n",
    "\n",
    "A subtility here is that some polygons are not directly extracted (field with `extract=False`), but should be kept for post-job actions. This requirement is filled by removing sub-dataframes that do not contain any extractable polyons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/users/Private/couchard/openeo-gfmap/src/openeo_gfmap/manager/job_splitters.py:51: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  polygons[\"h3index\"] = polygons.geometry.centroid.apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334 sub-datasets.\n",
      "236 sub-datasets after filtering sub-datasets with no point to extract.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "from openeo_gfmap.manager.job_splitters import split_job_hex\n",
    "\n",
    "base_df_path = \"https://artifactory.vgt.vito.be/artifactory/auxdata-public/gfmap/DEMO_CROPTYPE.gpkg\"\n",
    "base_df = gpd.read_file(base_df_path)\n",
    "# Splits the job using GFMAP\n",
    "split_jobs = split_job_hex(\n",
    "    base_df, max_points=60, grid_resolution=4\n",
    ")\n",
    "\n",
    "print(f'{len(split_jobs)} sub-datasets.')\n",
    "\n",
    "# Remove the geometry where there are no points with the \"extract\" flag\n",
    "split_jobs = [\n",
    "    job for job in split_jobs if job.extract.any()\n",
    "]\n",
    "print(f'{len(split_jobs)} sub-datasets after filtering sub-datasets with no point to extract.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second step: creating a dataframe for the GFMAP Job Manager\n",
    "\n",
    "Implementing a function that yields a `pandas.DataFrame` where each row correponds to a job.\n",
    "\n",
    "The dataframe should contain the informations required by the GFMAP Job Manager, as well as additional information used by the datacube creation function and the post-job action function.\n",
    "\n",
    "The output dataframe should be savable as a .csv file.\n",
    "\n",
    "Note: the full information of a sub-geodataframe of polygons can be saved into a row of a `pandas.DataFrame` by storing it in a row as string implementing the `geojson.FeatureCollection` interface. To convert the `geopandas.GeoDataFrame` into a stirng, simply use the `.to_json()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backend_name</th>\n",
       "      <th>out_prefix</th>\n",
       "      <th>out_extension</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2_10m</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2_10m</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2_10m</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2_10m</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2_10m</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2_10m</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2_10m</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2_10m</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2_10m</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>cdse</td>\n",
       "      <td>S2_10m</td>\n",
       "      <td>.nc</td>\n",
       "      <td>2020-08-30</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>{\"type\": \"FeatureCollection\", \"features\": [{\"i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>236 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    backend_name out_prefix out_extension  start_date    end_date  \\\n",
       "0           cdse     S2_10m           .nc  2020-08-30  2022-03-03   \n",
       "1           cdse     S2_10m           .nc  2020-08-30  2022-03-03   \n",
       "2           cdse     S2_10m           .nc  2020-08-30  2022-03-03   \n",
       "3           cdse     S2_10m           .nc  2020-08-30  2022-03-03   \n",
       "4           cdse     S2_10m           .nc  2020-08-30  2022-03-03   \n",
       "..           ...        ...           ...         ...         ...   \n",
       "231         cdse     S2_10m           .nc  2020-08-30  2022-03-03   \n",
       "232         cdse     S2_10m           .nc  2020-08-30  2022-03-03   \n",
       "233         cdse     S2_10m           .nc  2020-08-30  2022-03-03   \n",
       "234         cdse     S2_10m           .nc  2020-08-30  2022-03-03   \n",
       "235         cdse     S2_10m           .nc  2020-08-30  2022-03-03   \n",
       "\n",
       "                                              geometry  \n",
       "0    {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "1    {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "2    {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "3    {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "4    {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "..                                                 ...  \n",
       "231  {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "232  {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "233  {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "234  {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "235  {\"type\": \"FeatureCollection\", \"features\": [{\"i...  \n",
       "\n",
       "[236 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openeo_gfmap import Backend\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "def create_job_dataframe_s2(backend: Backend, split_jobs: List[gpd.GeoDataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"Create a dataframe from the split jobs, containg all the necessary information to run the job.\"\"\"\n",
    "    columns = ['backend_name', 'out_prefix', 'out_extension', 'start_date', 'end_date', 'geometry']\n",
    "    rows = []\n",
    "    for job in split_jobs:\n",
    "        # Compute the average in the valid date and make a buffer of 1.5 year around\n",
    "        median_time = pd.to_datetime(job.valid_date).mean()\n",
    "        start_date = median_time - pd.Timedelta(days=275)  # A bit more than 9 months\n",
    "        end_date = median_time + pd.Timedelta(days=275)  # A bit more than 9 months\n",
    "        \n",
    "        rows.append(\n",
    "            pd.Series(\n",
    "                dict(zip(columns, [backend.value, 'S2_10m', '.nc',  start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'), job.to_json()]))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "job_df = create_job_dataframe_s2(Backend.CDSE, split_jobs)\n",
    "\n",
    "job_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-sampling job dataframe to reduce execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a subset of the jobs to test the manager, the selected jobs have a fair amount of geometries to extract\n",
    "job_df = job_df.iloc[[0, 2, 3, -6]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geojson\n",
    "\n",
    "def get_job_nb_polygons(row: pd.Series) -> int:\n",
    "    \"\"\"Get the number of polygons in the geometry.\"\"\"\n",
    "    return len(list(filter(lambda feat: feat.properties.get(\"extract\"), geojson.loads(row.geometry)['features'])))\n",
    "\n",
    "job_df['nb_polygons'] = job_df.apply(get_job_nb_polygons, axis=1)\n",
    "job_df.nb_polygons.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third step: implement the datacube creator function.\n",
    "\n",
    "Implement a function to create, from the additional rows provided before, an `openeo.BatchJob` that will be used to run the job.\n",
    "\n",
    "In this case we extract Sentinel-2 data around a 64x64 pixel square of polygons which have the field `extract=True` (although we keep them in the row for the post-job action.)\n",
    "\n",
    "Note:\n",
    "Because the polygons to extract are specified in UTM dimensions (required to have a specific size), the dataset of polygon cannot be send directly through the openeo process graph (GeoJSON only support lat/lon coordinates). The sub-datasets of polygons are therefore uploaded to a publicly accessible URL so they can be used later by openeo during the execution of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openeo\n",
    "\n",
    "import requests\n",
    "from tempfile import NamedTemporaryFile\n",
    "import os\n",
    "import pandas as pd\n",
    "import geojson\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from openeo_gfmap import TemporalContext, Backend, BackendContext, FetchType, SpatialContext\n",
    "from openeo_gfmap.fetching import build_sentinel2_l2a_extractor\n",
    "\n",
    "def upload_geoparquet_artifactory(gdf: gpd.GeoDataFrame, row_id: int) -> str:\n",
    "    # Save the dataframe as geoparquet to upload it to artifactory\n",
    "    temporary_file = NamedTemporaryFile()\n",
    "    gdf.to_parquet(temporary_file.name)\n",
    "    \n",
    "    artifactory_username = os.getenv('ARTIFACTORY_USERNAME')\n",
    "    artifactory_password = os.getenv('ARTIFACTORY_PASSWORD')\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/octet-stream\"\n",
    "    }\n",
    "\n",
    "    upload_url = f\"https://artifactory.vgt.vito.be/artifactory/auxdata-public/gfmap-temp/openeogfmap_dataframe_{row_id}.parquet\"\n",
    "\n",
    "    with open(temporary_file.name, 'rb') as f:\n",
    "        response = requests.put(upload_url, headers=headers, data=f, auth=(artifactory_username, artifactory_password))\n",
    "\n",
    "    assert response.status_code == 201, f\"Error uploading the dataframe to artifactory: {response.text}\"\n",
    "\n",
    "    return upload_url\n",
    "\n",
    "\n",
    "def create_datacube_s2(row: pd.Series, connection: openeo.DataCube, provider=None, connection_provider=None) -> openeo.BatchJob:\n",
    "\n",
    "    def buffer_geometry(geometry: geojson.FeatureCollection, buffer: int) -> gpd.GeoDataFrame:\n",
    "        gdf = gpd.GeoDataFrame.from_features(geometry).set_crs(epsg=4326)\n",
    "        utm = gdf.estimate_utm_crs()\n",
    "        gdf = gdf.to_crs(utm)\n",
    "\n",
    "        gdf['geometry'] = gdf.centroid.apply(\n",
    "            # Clips the point to the closest 20m from the S2 grid\n",
    "            lambda point: Point(round(point.x / 20.0) * 20.0, round(point.y / 20.0) * 20.0)\n",
    "        ).buffer(distance=buffer, cap_style=3)\n",
    "\n",
    "        return gdf\n",
    "\n",
    "    def filter_extractonly_geometries(collection: geojson.FeatureCollection):\n",
    "        # Filter out geometries that do not have the field extract=True\n",
    "        features = [f for f in collection.features if f.properties.get('extract', False)]\n",
    "        return geojson.FeatureCollection(features)\n",
    "\n",
    "    start_date = row.start_date\n",
    "    end_date = row.end_date\n",
    "    temporal_context = TemporalContext(start_date, end_date)\n",
    "\n",
    "    # Get the feature collection containing the geometry to the job\n",
    "    geometry = geojson.loads(row.geometry)\n",
    "    assert isinstance(geometry, geojson.FeatureCollection)\n",
    "\n",
    "    # Filter the geometry to the rows with the extract only flag\n",
    "    geometry = filter_extractonly_geometries(geometry)\n",
    "    assert len(geometry.features) > 0, \"No geometries with the extract flag found\"\n",
    "\n",
    "    # Performs a buffer of 64 px around the geometry\n",
    "    geometry_df = buffer_geometry(geometry, 320)\n",
    "    spatial_extent_url = upload_geoparquet_artifactory(geometry_df, row.name)\n",
    "\n",
    "    # Backend name and fetching type\n",
    "    backend = Backend(row.backend_name)\n",
    "    backend_context = BackendContext(backend)\n",
    "\n",
    "    fetch_type = FetchType.POLYGON\n",
    "    bands_to_download = ['S2-L2A-B01', 'S2-L2A-B02', 'S2-L2A-B03', 'S2-L2A-B04', 'S2-L2A-B05', 'S2-L2A-B06', 'S2-L2A-B07', 'S2-L2A-B08', 'S2-L2A-B8A', 'S2-L2A-B09', 'S2-L2A-B11', 'S2-L2A-B12', 'S2-L2A-SCL']\n",
    "\n",
    "    # Create the job to extract S2\n",
    "    extraction_parameters = {\n",
    "        \"target_resolution\": 10,\n",
    "        \"load_collection\": {\n",
    "            \"eo:cloud_cover\": lambda val: val <= 95.0,\n",
    "        },\n",
    "    }\n",
    "    extractor = build_sentinel2_l2a_extractor(\n",
    "        backend_context, bands=bands_to_download, fetch_type=fetch_type.POLYGON, **extraction_parameters \n",
    "    )\n",
    "\n",
    "    cube = extractor.get_cube(connection, spatial_extent_url, temporal_context)\n",
    "\n",
    "    # Compute the SCL dilation and add it to the cube\n",
    "    scl_dilated_mask = cube.process(\n",
    "        \"to_scl_dilation_mask\",\n",
    "        data=cube,\n",
    "        scl_band_name=\"S2-L2A-SCL\",\n",
    "        kernel1_size=17,  # 17px dilation on a 20m layer\n",
    "        kernel2_size=77,   # 77px dilation on a 20m layer\n",
    "        mask1_values=[2, 4, 5, 6, 7],\n",
    "        mask2_values=[3, 8, 9, 10, 11],\n",
    "        erosion_kernel_size=3\n",
    "    ).rename_labels(\"bands\", [\"S2-L2A-SCL_DILATED_MASK\"])\n",
    "\n",
    "    cube = cube.merge_cubes(scl_dilated_mask)\n",
    "\n",
    "    # Compute the distance to cloud and add it to the cube\n",
    "    scl = cube.filter_bands(['S2-L2A-SCL'])\n",
    "    distance_to_cloud = scl.apply_neighborhood(\n",
    "        process=openeo.UDF.from_file('udf_distance_to_cloud.py'),\n",
    "        size=[{'dimension': 'x', 'unit': 'px', 'value': 256}, {'dimension': 'y', 'unit': 'px', 'value': 256}],\n",
    "        overlap=[{'dimension': 'x', 'unit': 'px', 'value': 16}, {'dimension': 'y', 'unit': 'px', 'value': 16}]\n",
    "    ).rename_labels('bands', ['S2-L2A-DISTANCE-TO-CLOUD'])\n",
    "\n",
    "    cube = cube.merge_cubes(distance_to_cloud)\n",
    "    cube = cube.linear_scale_range(0, 65534, 0, 65534)\n",
    "\n",
    "    # Get the h3index to use in the tile\n",
    "    h3index = geometry.features[0].properties['h3index']\n",
    "    valid_date = geometry.features[0].properties['valid_date']\n",
    "\n",
    "    # Increase the memory of the jobs depending on the number of polygons to extract\n",
    "    number_polygons = get_job_nb_polygons(row)\n",
    "    _log.debug(f\"Number of polygons to extract: {number_polygons}\")\n",
    "\n",
    "    job_options = {\n",
    "        \"executor-memory\": \"5G\",\n",
    "        \"executor-memoryOverhead\": \"2G\",\n",
    "    }\n",
    "\n",
    "    return cube.create_job(\n",
    "        out_format=\"NetCDF\",\n",
    "        title=f\"GFMAP_Extraction_S2_{h3index}_{valid_date}\",\n",
    "        sample_by_feature=True,\n",
    "        job_options=job_options\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth step: create output paths\n",
    "\n",
    "Implement a function that from a temporary path containing a job result, from the job dataframe row and the root folder will choose the output path where to save that job result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.62 s, sys: 85.2 ms, total: 4.71 s\n",
      "Wall time: 5.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load the S2 grid\n",
    "s2_grid = gpd.read_file(\"https://artifactory.vgt.vito.be/artifactory/auxdata-public/gfmap/s2grid_bounds.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "from pyproj import Transformer, CRS\n",
    "from shapely.geometry import box, Point\n",
    "\n",
    "def generate_output_path_s2(root_folder: Path, tmp_path: Path, geometry_index: int, row: pd.Series):\n",
    "    features = geojson.loads(row.geometry)\n",
    "    sample_id = features[geometry_index].properties['sample_id']\n",
    "    ref_id = features[geometry_index].properties['ref_id']\n",
    "    \n",
    "    # Loads the array lazily in-memory\n",
    "    try:\n",
    "        inds = xr.open_dataset(tmp_path, chunks='auto')\n",
    "        \n",
    "        source_crs = CRS.from_wkt(inds.crs.attrs['crs_wkt'])\n",
    "        dst_crs = CRS.from_epsg(4326)\n",
    "        \n",
    "        transformer = Transformer.from_crs(source_crs, dst_crs, always_xy=True)\n",
    "\n",
    "        # Get the center point of the tile\n",
    "        centroid_utm = box(\n",
    "            inds.x.min().item(), inds.y.min().item(), inds.x.max().item(), inds.y.max().item()\n",
    "        ).centroid\n",
    "        centroid_latlon = Point(*transformer.transform(centroid_utm.x, centroid_utm.y))\n",
    "\n",
    "        # Intersecting with the s2 grid\n",
    "        intersecting = s2_grid.geometry.intersects(centroid_latlon)\n",
    "\n",
    "        # Select the intersecting cell that has a centroid the closest from the point\n",
    "        intersecting_cells = s2_grid[intersecting]\n",
    "        intersecting_cells['distance'] = intersecting_cells.distance(centroid_latlon)\n",
    "        intersecting_cells.sort_values('distance', inplace=True)\n",
    "        s2_tile = intersecting_cells.iloc[0]\n",
    "\n",
    "        s2_tile_id = s2_tile.tile\n",
    "\n",
    "        subfolder = root_folder / ref_id / str(source_crs.to_epsg()) / s2_tile_id / sample_id\n",
    "    except Exception:\n",
    "        subfolder = root_folder / 'unsortable'\n",
    "\n",
    "    return subfolder / f'{row.out_prefix}_{sample_id}_{source_crs.to_epsg()}_{row.start_date}_{row.end_date}{row.out_extension}'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fifth step: Define the post-job action\n",
    "\n",
    "The post-job action will be called once the job resut was downloaded and saved to a specific path.\n",
    "\n",
    "A post-job action function must receive 3 parameters:\n",
    "* `job_items`: STAC items containing the currently extracted data for the job.\n",
    "* `row`: The current job dataframe row.\n",
    "* `parameters`: User-defined parameters set in the `GFMAPJobManager` constructor.\n",
    "\n",
    "The post-job action must return back the list of job items. The user is responsible for updating,\n",
    "adding and removing the items. For example, in this case the user creates a raster file with\n",
    "ground truth data, the user then adds an asset using the predefined `openeo_gfmap.stac.AUXILIARY`\n",
    "AssetDefintion to the related item, pointing to the generated NetCDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:16: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:16: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "/tmp/ipykernel_4054/2115750546.py:16: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert (\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "from datetime import datetime\n",
    "\n",
    "from openeo_gfmap.stac import AUXILIARY\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.transform import from_bounds\n",
    "import pystac\n",
    "import json\n",
    "\n",
    "def add_item_asset(related_item: pystac.Item, path: Path):\n",
    "    asset = AUXILIARY.create_asset(\n",
    "        href=path.as_posix()\n",
    "    )\n",
    "    related_item.add_asset('auxiliary', asset)\n",
    "\n",
    "def post_job_action(job_items: List[pystac.Item], row: pd.Series, parameters: dict = {}) -> list:\n",
    "    base_gpd = gpd.GeoDataFrame.from_features(json.loads(row.geometry)).set_crs(epsg=4326)\n",
    "    assert (\n",
    "        len(base_gpd[base_gpd.extract == True]) == len(job_items),\n",
    "        \"The number of result paths should be the same as the number of geometries\"\n",
    "    )\n",
    "    extracted_gpd = base_gpd[base_gpd.extract == True].reset_index(drop=True)\n",
    "    # In this case we want to burn the metadata in a new file in the same folder as the S2 product\n",
    "    for idx, item in enumerate(job_items):\n",
    "        sample_id = extracted_gpd.iloc[idx].sample_id\n",
    "        ref_id = extracted_gpd.iloc[idx].ref_id\n",
    "        confidence = extracted_gpd.iloc[idx].confidence\n",
    "        valid_date = extracted_gpd.iloc[idx].valid_date\n",
    "\n",
    "        item_asset_path = Path(\n",
    "            list(item.assets.values())[0].href\n",
    "        )\n",
    "        # Read information from the item file (could also read it from the item object metadata)\n",
    "        result_ds = xr.open_dataset(item_asset_path, chunks='auto')\n",
    "\n",
    "        # Add some metadata to the result_df netcdf file\n",
    "        result_ds.attrs.update({\n",
    "            'start_date': row.start_date,\n",
    "            'end_date': row.end_date,\n",
    "            'valid_date': valid_date,\n",
    "            'GFMAP_version': version('openeo_gfmap'),\n",
    "            'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'description': f'Sentinel2 L2A observations for sample: {sample_id}, unprocessed.',\n",
    "            'title': f'Sentinel2 L2A - {sample_id}',\n",
    "            'sample_id': sample_id,\n",
    "            'spatial_resolution': '10m'\n",
    "        })\n",
    "        result_ds.to_netcdf(item_asset_path, format='NETCDF4', engine='h5netcdf')\n",
    "\n",
    "        target_crs = CRS.from_wkt(result_ds.crs.attrs['crs_wkt'])\n",
    "\n",
    "        # Get the surrounding polygons around our extracted center geometry to rastetize them\n",
    "        bounds = (\n",
    "            result_ds.x.min().item(),\n",
    "            result_ds.y.min().item(),\n",
    "            result_ds.x.max().item(),\n",
    "            result_ds.y.max().item()\n",
    "        )\n",
    "        bbox = box(*bounds)\n",
    "        surround_gpd = base_gpd.to_crs(target_crs).clip(bbox)\n",
    "\n",
    "        # Burn the polygon croptypes\n",
    "        transform = from_bounds(*bounds, result_ds.x.size, result_ds.y.size)\n",
    "        croptype_shapes = list(zip(surround_gpd.geometry, surround_gpd.croptype_label))\n",
    "\n",
    "        fill_value = 0\n",
    "        croptype = rasterize(\n",
    "            croptype_shapes,\n",
    "            out_shape=(result_ds.y.size, result_ds.x.size),\n",
    "            transform=transform,\n",
    "            all_touched=False,\n",
    "            fill=fill_value,\n",
    "            default_value=0,\n",
    "            dtype='int64'\n",
    "        )\n",
    "\n",
    "        # Create the attributes to add to the metadata\n",
    "        attributes = {\n",
    "            'ref_id': ref_id,\n",
    "            'sample_id': sample_id,\n",
    "            'confidence': str(confidence),\n",
    "            'valid_date': valid_date,\n",
    "            '_FillValue': fill_value,\n",
    "            'Conventions': 'CF-1.9',\n",
    "            'GFMAP_version': version('openeo_gfmap'),\n",
    "            'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'description': f'Contains rasterized WorldCereal labels for sample: {sample_id}.',\n",
    "            'title': f'WORLDCEREAL Auxiliary file for sample: {sample_id}',\n",
    "            'spatial_resolution': '10m'\n",
    "        }\n",
    "\n",
    "        aux_dataset = xr.Dataset(\n",
    "            {'LABEL': (('y', 'x'), croptype), 'crs': result_ds['crs']},\n",
    "            coords={'y': result_ds.y, 'x': result_ds.x},\n",
    "            attrs=attributes\n",
    "        )\n",
    "        # Required to map the 'crs' layer as geo-reference for the 'LABEL' layer.\n",
    "        aux_dataset['LABEL'].attrs['grid_mapping'] = 'crs'\n",
    "\n",
    "        # Save the metadata in the same folder as the S2 product\n",
    "        metadata_path = (\n",
    "            item_asset_path.parent / \n",
    "            f'WORLDCEREAL_10m_{sample_id}_{target_crs.to_epsg()}_{valid_date}.nc'\n",
    "        )\n",
    "        aux_dataset.to_netcdf(metadata_path, format='NETCDF4', engine='h5netcdf')\n",
    "        aux_dataset.close()\n",
    "\n",
    "        # Adds this metadata as a new asset\n",
    "        add_item_asset(item, metadata_path)\n",
    "        \n",
    "    return job_items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sixth and last step: Running the manager\n",
    "\n",
    "Let's initialize and execute the Job Manager as defined the GFMAP, and then run it using the functions defined previously\n",
    "\n",
    "STAC related parameters such as `collection_id` and `collection_description` are also required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openeo_gfmap.manager.job_manager import GFMAPJobManager\n",
    "from openeo_gfmap.backend import cdse_staging_connection\n",
    "\n",
    "\n",
    "base_output_dir = Path('/data/users/Public/couchard/world_cereal/extractions_5/')\n",
    "tracking_job_csv = base_output_dir / 'job_tracker.csv'\n",
    "\n",
    "manager = GFMAPJobManager(\n",
    "    output_dir=base_output_dir,\n",
    "    output_path_generator=generate_output_path_s2,\n",
    "    collection_id='SENTINEL-EXTRACTION',\n",
    "    collection_description=(\n",
    "        \"Sentinel-2 and Auxiliary data extraction example.\"\n",
    "    ),\n",
    "    post_job_action=post_job_action,\n",
    "    poll_sleep=60,\n",
    "    n_threads=2,\n",
    "    post_job_params={}\n",
    ")\n",
    "\n",
    "manager.add_backend(\n",
    "    Backend.CDSE_STAGING.value, cdse_staging_connection, parallel_jobs=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 14:32:38,392|openeo_gfmap.manager|INFO:  Starting job manager using 2 worker threads.\n",
      "2024-03-07 14:32:38,395|openeo_gfmap.manager|INFO:  Workers started, creating and running jobs.\n",
      "2024-03-07 14:32:38,592|openeo_gfmap.manager|DEBUG:  Normalizing dataframe. Columns: Index(['backend_name', 'out_prefix', 'out_extension', 'start_date', 'end_date',\n",
      "       'geometry', 'nb_polygons', 'status', 'id', 'start_time', 'cpu',\n",
      "       'memory', 'duration', 'description', 'costs'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated using refresh token.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 14:32:40,375|openeo_gfmap.manager|DEBUG:  Number of polygons to extract: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataCube(<PGNode 'dimension_labels' at 0x7fb2dd7cfa40>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 14:34:05,989|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:35:06,713|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:36:07,149|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:37:08,393|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:38:08,991|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:39:09,497|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:40:11,296|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:41:12,353|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:42:13,100|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:43:13,535|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:44:14,023|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:45:14,444|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:46:14,884|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:47:15,338|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:48:15,855|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:49:18,136|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:50:19,075|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:51:19,412|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:52:19,838|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:53:21,069|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:54:21,621|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:55:22,007|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:56:22,363|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is running (on backend cdse-staging).\n",
      "2024-03-07 14:57:23,108|openeo_gfmap.manager|DEBUG:  Status of job j-240307f2186344ee9bed9c649679dc92 is finished (on backend cdse-staging).\n",
      "2024-03-07 14:57:23,110|openeo_gfmap.manager|INFO:  Job j-240307f2186344ee9bed9c649679dc92 finished successfully, queueing on_job_done...\n",
      "2024-03-07 14:57:23,115|openeo_gfmap.manager|DEBUG:  Worker thread Thread-5: polled finished job with status PostJobStatus.FINISHED.\n",
      "2024-03-07 14:57:24,650|openeo_gfmap.manager|DEBUG:  Downloading asset openEO.nc from job j-240307f2186344ee9bed9c649679dc92 -> /tmp/tmpyu7z9urd\n",
      "2024-03-07 14:57:43,859|openeo_gfmap.manager|DEBUG:  Generating output path for asset openEO.nc from job j-240307f2186344ee9bed9c649679dc92...\n",
      "/tmp/ipykernel_4054/3116825532.py:31: UserWarning: Geometry is in a geographic CRS. Results from 'distance' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  intersecting_cells['distance'] = intersecting_cells.distance(centroid_latlon)\n",
      "/home/couchard/miniconda3/envs/gfmap/lib/python3.9/site-packages/geopandas/geodataframe.py:1543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "/tmp/ipykernel_4054/3116825532.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  intersecting_cells.sort_values('distance', inplace=True)\n",
      "2024-03-07 14:57:44,390|openeo_gfmap.manager|DEBUG:  Generated path for asset openEO.nc from job j-240307f2186344ee9bed9c649679dc92 -> /data/users/Public/couchard/world_cereal/extractions_5/2021_EUR_DEMO_POLY_110/32635/35VMD/2021_LV_LPIS_POLY_110-12880341/S2_10m_2021_LV_LPIS_POLY_110-12880341_32635_2020-08-30_2022-03-03.nc\n",
      "2024-03-07 14:57:48,864|openeo_gfmap.manager|INFO:  Downloaded asset openEO.nc from job j-240307f2186344ee9bed9c649679dc92 -> /data/users/Public/couchard/world_cereal/extractions_5/2021_EUR_DEMO_POLY_110/32635/35VMD/2021_LV_LPIS_POLY_110-12880341/S2_10m_2021_LV_LPIS_POLY_110-12880341_32635_2020-08-30_2022-03-03.nc\n",
      "2024-03-07 14:57:54,824|openeo_gfmap.manager|INFO:  Parsed item j-240307f2186344ee9bed9c649679dc92_openEO.nc from job j-240307f2186344ee9bed9c649679dc92\n",
      "2024-03-07 14:57:54,826|openeo_gfmap.manager|DEBUG:  Calling post job action for job j-240307f2186344ee9bed9c649679dc92...\n",
      "2024-03-07 14:57:55,644|openeo_gfmap.manager|INFO:  Added 1 items to the STAC collection.\n",
      "2024-03-07 14:57:55,646|openeo_gfmap.manager|INFO:  Job j-240307f2186344ee9bed9c649679dc92 and post job action finished successfully.\n"
     ]
    }
   ],
   "source": [
    "# Run the jobs and create the STAC catalogue\n",
    "manager.run_jobs(job_df, create_datacube_s2, tracking_job_csv)\n",
    "manager.create_stac()  # By default the STAC catalogue will be saved in the stac subfolder of the base_output_dir folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfmap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
